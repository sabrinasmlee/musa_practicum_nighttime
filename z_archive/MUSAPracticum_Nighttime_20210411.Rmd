---
title: "MUSA Practicum Nighttime Economy"
author: "Maddy Kornhauser, Brian Rawn, Sabrina Lee"
date: "4/3/2021"
output: 
  html_document: 
        code_folding: hide
        toc: true
        toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	out.width = "75%"
)
```

# 0. Weekly Updates 4/13/2021

This week, we have continued to refine the predictive model and continued to move forward on the app. Major updates on the model since last week.

* **New Features**: We've added a handful of new features that have improved the model. These include:
  + Trolley and busstop features from SEPTA.
  + Retail mix features. We've added up the number of business categories along the corridor (both from the top_category variable and the sub_category variable). These ended up being very significant variables.
  + Dummy open late feature indicating if a business close after 6PM.
  
* **Outliers**: We have continued to play around with removing outliers from the dataset. As we show below, we have removed Market East (which the model was drastically under-predicting) and certain observations that had a high count of night visits per square mile. These models demonstrated marginal improvement.

* **Predicting Daytime Trips**: Per our meeting with Michael on 4/9, we changed the dependent variable in our model to predict *daytime visits* rather than nighttime visits. We found that our model had substantially higher errors when predicting traffic during the daytime. 

We plan to meet with Matt on the Monday evening (4/12) before class to further discuss our approach to the model.

This week, we also worked on the database .csv that will underly the app.

# 1. Use Case Development

## Purpose

Our goal is to help business improvement districts (BIDs) and small businesses understand the patterns of nightlife across Philadelphia’s commercial corridors in order to further recovery efforts following the COVID-19 pandemic. 

## The Tool

An interactive data dashboard that allows users to understand nightlife patterns at the corridor level across Philadelphia. This dashboard would, for each corridor, answer the questions of “who, what, where, and when?” of nightlife visitors. The dashboard would also allow for comparisons to other corridors. Potential metrics that would be displayed include:

* Corridor popularity (volume of trips) over time. 
* The percentage of visitors attributable to different origin census tracts and a breakdown of associated demographic characteristics.
* Average distance traveled to restaurants/bars/theaters.
* Average dwell time by commercial use.
* The percentage drop in trips that restaurants/bars/theaters experienced as a result of COVID-19.
* For each, a comparison to the average of other commercial corridors.

In addition, the tool will have the additional functionality of being able to predict the future popularity of a commercial corridor given changes in time (hour, day, month), weather, volume of commercial establishments (count or floor area), and retail mix. This predictive functionality would help inform decisions relating to the regulation and expansion of nightlife activities.

## Applications

How will this tool be utilized for economic development? Potential applications include use by BIDs and small businesses to:

* Understand peak trip times by use and location in order to inform parking, transit, or commercial policies.
* Understand relative popularity of commercial corridors to inform decisions to grant licenses or small business assistance.
* Understand origins and demographics of visitors to more effectively target marketing resources. 
* Understand the future effect of more commercial establishments or square footage in a given commercial corridor.

# 2. The SafeGraph Dataset

Safegraph uses anonymized cell phone GPS data to record trips to commercial points of interest. This data can tell us from where a trip was made, what time the trip was made, and how long the individual stated at the point of interest.

* Pros: 
  + Brand new dataset. 
  + Lots of unexplored applications and potential insights

* Cons: 
  + Data is new and has a significant amount of incorrectly attributed trips, especially in urban areas

# 3. Exploratory Data Analysis

```{r load packages, include=FALSE}
library(tidyverse)
library(tidycensus)
library(sf)
library(lubridate)
library(datetime)
library(viridis)
library(gridExtra)
library(kableExtra)
library(dplyr)
library(ggplot2)
library(gganimate)
library(FNN)
library(caret)
library(stargazer)
library(ranger)

qBr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                 c(.01,.2,.4,.6,.8), na.rm=T)
  }
}
q5 <- function(variable) {as.factor(ntile(variable, 5))}
palette3 <- viridis_pal()(3)
palette5 <- viridis_pal()(5)
mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2)
  )
}
plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 16,colour = "black"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}
nn_function <- function(measureFrom,measureTo,k) {
  measureFrom_Matrix <- as.matrix(measureFrom)
  measureTo_Matrix <- as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
  output <-
    as.data.frame(nn) %>%
    rownames_to_column(var = "thisPoint") %>%
    gather(points, point_distance, V1:ncol(.)) %>%
    arrange(as.numeric(thisPoint)) %>%
    group_by(thisPoint) %>%
    dplyr::summarize(pointDistance = mean(point_distance)) %>%
    arrange(as.numeric(thisPoint)) %>% 
    dplyr::select(-thisPoint) %>%
    pull()
  
  return(output)  
}

setwd("~/GitHub/musa_practicum_nighttime")
```

```{r load & wrangle data}
dat <- read.csv("./data/moves_2018.csv")
phila <- st_read("./demo/phila.geojson", quiet = TRUE)

dat2 <- dat %>% 
  dplyr::select(safegraph_place_id, 
                date_range_start, 
                date_range_end, 
                raw_visit_counts,
                raw_visitor_counts, 
                visits_by_day, 
                poi_cbg, 
                visitor_home_cbgs, 
                visitor_daytime_cbgs, 
                visitor_work_cbgs, 
                visitor_country_of_origin,
                distance_from_home, 
                median_dwell, 
                bucketed_dwell_times, 
                related_same_day_brand, 
                related_same_month_brand, 
                popularity_by_hour, 
                popularity_by_day, 
                device_type) %>%
  left_join(., phila, by = "safegraph_place_id") %>% 
  st_as_sf() %>%
  st_transform('ESRI:102728')

#Block group shapefiles (projected & unprojected)
phl_cbg <- 
  st_read("http://data.phl.opendata.arcgis.com/datasets/2f982bada233478ea0100528227febce_0.geojson", quiet = TRUE) %>%
  mutate(GEOID10 = as.numeric(GEOID10))%>%
  st_transform('ESRI:102728') 
phl_cbg_unproj <- 
  st_read("http://data.phl.opendata.arcgis.com/datasets/2f982bada233478ea0100528227febce_0.geojson", quiet = TRUE) %>%
  mutate(GEOID10 = as.numeric(GEOID10),
         Lat = as.numeric(INTPTLAT10),
         Lon = as.numeric(INTPTLON10))

#Boundaries shapefiles (projected & unprojected)
phl_boundary <- 
  st_read("https://opendata.arcgis.com/datasets/405ec3da942d4e20869d4e1449a2be48_0.geojson", quiet = TRUE)%>%
  st_transform('ESRI:102728')

phl_boundary_unproj <- 
  st_read("https://opendata.arcgis.com/datasets/405ec3da942d4e20869d4e1449a2be48_0.geojson", quiet = TRUE)

#Corridor shapefiles (projected & unprojected)
phl_corridors <- st_read("http://data.phl.opendata.arcgis.com/datasets/f43e5f92d34e41249e7a11f269792d11_0.geojson", quiet = TRUE)%>%
  st_transform('ESRI:102728') %>%
  mutate(corr_type = ifelse(CORRIDOR_TYPE == 1, "1. Neighborhood Subcenter", 
                            ifelse(CORRIDOR_TYPE == 2, "2. Neighborhood Center", 
                                   ifelse(CORRIDOR_TYPE == 3, "3. Community Center", 
                                          ifelse(CORRIDOR_TYPE == 4, "4. Regional Center", 
                                                 ifelse(CORRIDOR_TYPE == 5, "5. Superregional Center", 
                                                        ifelse(CORRIDOR_TYPE == 6, "6. Speciality Center", "Other")))))))

phl_corridors_unproj <- st_read("http://data.phl.opendata.arcgis.com/datasets/f43e5f92d34e41249e7a11f269792d11_0.geojson", quiet = TRUE)

#Neighborhood shapefiles (projected & unprojected)
phl_nhoods <- 
  st_read("https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson", 
          quiet = TRUE) %>%
  st_transform('ESRI:102728')
phl_nhoods_unproj <- 
  st_read("https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson", 
          quiet = TRUE)

#Planning district shapefiles (projected & unprojected)
phl_dist <-
  st_read("http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson", 
          quiet = TRUE) %>%
  st_transform('ESRI:102728')

phl_dist_unproj <-
  st_read("http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson", 
          quiet = TRUE)

#Philadelphia zip codes
phl_zip <- 
  st_read("http://data.phl.opendata.arcgis.com/datasets/b54ec5210cee41c3a884c9086f7af1be_0.geojson", quiet = TRUE) %>%
  st_transform('ESRI:102728') %>%
  mutate(CODE = as.numeric(CODE))
```

## Philadelphia nightlife establishments

Our first research question is where Philadelphia nightlife establishments are located across the city. The following maps indicate where businesses that contribute to the city's nightlife economy are located. The categories include:

* Bars (Drinking Places)
* Restaurants 
* Arts Venues (Promotes of Performing Arts)

Figure X.X below shows the spatial patterns of the business categories. Bars and restaurants represent the highest number of businesses which are spread across the city. Hotels are mostly clustered in the central district of the city and near the airport in the southwest portion of the city. There are far fewer casinos and arts venues.

Throughout the analysis, we pay special attention to bars and restaurants, as these organizations are well distributed throughout the city and apply to a local Philadelphia customer base.

```{r establishment locations}
dat2 %>%
  filter(top_category == "Drinking Places (Alcoholic Beverages)" |
           top_category == "Restaurants and Other Eating Places" |
           # top_category == "Traveler Accommodation" |
           # top_category == "Gambling Industries" |
           top_category == "Promoters of Performing Arts, Sports, and Similar Events" |
           top_category == "Performing Arts Companies") %>%
  mutate(category = case_when(top_category == "Drinking Places (Alcoholic Beverages)" ~ "Bars",
                              top_category == "Restaurants and Other Eating Places" ~ "Restaurants",
                              top_category == "Promoters of Performing Arts, Sports, and Similar Events" ~ "Arts",
                              top_category == "Performing Arts Companies" ~ "Arts")) %>%
  ggplot() + 
  geom_sf(data = phl_cbg, fill = "grey80", color = "transparent") +
  geom_sf(color = "red", size = .1) +
  labs(title = "Location of Nightlife Establishments",
       subtitle = "Figure X.X") +
  facet_wrap(~category, nrow = 1) +
  mapTheme()
```

To look at the spatial patterns another way, we review the distribution of businesses  with a fishnet grid. The fishnet allows us to visualize clusters and hotspots. Starting first with restaurants, Figure X.X below demonstrates the spatial patterns of these businesses.  Though restaurants are well distributed throughout the city, there appears to be a higher concentration of yellow cells (indicating a higher count of restaurants) in the central part of the city. This corresponds to what we observe in the point data analysis. 
```{r unnesting hour dataset}
#Unnesting popularity by hour variable
dat_hour <- 
  dat2 %>% 
  select(safegraph_place_id, top_category, sub_category, popularity_by_hour, poi_cbg, median_dwell) %>%
  mutate(popularity_by_hour = str_remove_all(popularity_by_hour, pattern = "\\[|\\]")) %>%
  unnest(popularity_by_hour) %>%
  separate(.,
           popularity_by_hour,
           c("0", "1", "2", "3", "4", "5", "6", 
             "7", "8", "9", "10", "11", "12", 
             "13", "14", "15", "16", "17", "18",
             "19", "20", "21", "22", "23"),
           sep = ",") %>%
  pivot_longer(cols = 4:27,
               names_to = "Hour",
               values_to = "Count") %>%
  mutate(Hour = as.numeric(Hour),
         Count = as.numeric(Count))

#Creating a fishnet
phillyBoundary <- 
  phl_zip %>%
  select(geometry) %>%
  st_union() %>%
   st_transform('ESRI:102728') %>% 
  st_as_sf() 

fishnet <- 
  st_make_grid(phillyBoundary, cellsize = 1500, square = FALSE) %>%
  .[phillyBoundary] %>% #MK added this line
  st_sf() %>%
  mutate(uniqueID = rownames(.))

# #Plot fishnet
# ggplot() +
#   geom_sf(data = fishnet, fill = "#440255", color = "black")

```

Figure X.X below includes other sectors that contribute to Philadelphia's nightlife economy such as hotels and casinos. This figure combines the fishnets into a single panel that allows us to compare distribution of industries across business type.
```{r}
#Filter restaurants
restaurants <- dat2 %>%
  filter(top_category == "Restaurants and Other Eating Places")%>%
    st_transform('ESRI:102728')%>%
  mutate(Legend = "Restaurants")

#aggregate restaurant count by fishnet cell
restaurants_net <-
  dplyr::select(restaurants) %>% 
  mutate(countRestaurants = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countRestaurants = replace_na(countRestaurants, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

#Bars
bars <- dat2 %>%
  filter(top_category == "Drinking Places (Alcoholic Beverages)")%>%
    st_transform('ESRI:102728')%>%
  mutate(Legend = "Bars")

#aggregate bars by fishnet cell
bars_net <-
  dplyr::select(bars) %>% 
  mutate(countBars = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countBars = replace_na(countBars, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE)) %>%
  mutate(Legend = "Bars")

# #Casinos
# casinos <- dat2 %>%
#   filter(top_category == "Gambling Industries") %>%
#   st_transform('ESRI:102728') %>%
#   mutate(Legend = "Casinos")
# 
# casinos_net <-
#   dplyr::select(casinos) %>% 
#   mutate(countCasinos = 1) %>% 
#   aggregate(., fishnet, sum) %>%
#   mutate(countCasinos = replace_na(countCasinos, 0),
#          uniqueID = rownames(.),
#          cvID = sample(round(nrow(fishnet) / 24), 
#                        size=nrow(fishnet), replace = TRUE))

#Performing arts
performingarts <- dat2 %>%
  filter(top_category == "Promoters of Performing Arts, Sports, and Similar Events" |
           top_category == "Performing Arts Companies") %>%
  st_transform('ESRI:102728') %>%
  mutate(Legend = "Performing Arts")

performingarts_net <-
  dplyr::select(performingarts) %>% 
  mutate(countPerformingarts = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countPerformingarts = replace_na(countPerformingarts, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

# #Hotels
# hotels <- dat2 %>%
#   filter(top_category == "Traveler Accommodation") %>%
#   st_transform('ESRI:102728') %>%
#   mutate(Legend = "Hotels")
# 
# hotels_net <-
#   dplyr::select(hotels) %>% 
#   mutate(countHotels = 1) %>% 
#   aggregate(., fishnet, sum) %>%
#   mutate(countHotels = replace_na(countHotels, 0),
#          uniqueID = rownames(.),
#          cvID = sample(round(nrow(fishnet) / 24), 
#                        size=nrow(fishnet), replace = TRUE))

# Combining fishnets into a single dataframe
vars_net <- 
  rbind(restaurants, 
        bars, 
        performingarts
        # , 
        # casinos, 
        # hotels
        ) %>%
  st_join(., fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  dplyr::summarize(count = n()) %>%
    full_join(fishnet) %>%
    spread(Legend,count, fill=0) %>%
    st_sf() %>%
    dplyr::select(-`<NA>`) %>%
    na.omit() %>%
    ungroup()

vars_net.long <- 
  gather(vars_net, Variable, value, -geometry, -uniqueID)

vars <- unique(vars_net.long$Variable)
mapList <- list()

#Plotting small multiple maps
for(i in vars){
  mapList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(vars_net.long, Variable == i), aes(fill=value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme()}

do.call(grid.arrange, c(mapList, ncol=3, top="Count of Nightlife Businesses per Fishnet", bottom = "Figure X.X"))
```

## Nightlife hours

Next, we explore when visitors make trips to nightlife establishments. To do this, we worked with the popularity_by_hour SafeGraph variable, which sums the total number of visitors by hour for each month.

Figure X.X shows the average foot traffic by business type over the course of a day. This graphic indicates that though certain business types are considererd part of the nightlife economy, they do not exclusively experience traffic in the evenings. Restaurants are a good example of this, where the data indicates that the highest levels of traffic occur in the middle of the day. Arts venues, on the other hand, have a clear patterns indicating that they are more popular later in the day.
```{r traffic by nightlife establishment}
dat_hour %>%
  filter(top_category == "Drinking Places (Alcoholic Beverages)" |
           top_category == "Restaurants and Other Eating Places" |
           # top_category == "Traveler Accommodation" |
           # top_category == "Gambling Industries" |
           top_category == "Promoters of Performing Arts, Sports, and Similar Events" |
           top_category == "Performing Arts Companies") %>%
  group_by(Hour, top_category) %>%
  dplyr::summarize(Count = mean(Count)) %>%
  ggplot(., aes(x = Hour, y = Count)) + 
  geom_col() +
  labs(title = "Philadelphia Nightlife Organizations, Average Traffic by Hour",
       subtitle = "Figure X.X") +
  facet_wrap(~top_category, scales = "free") +
  plotTheme()
```

The following animation shows restaurant visitor counts by census block group over the course of the day. We observe the highest amount of traffic in center city in the middle of the day and then a resurgence of traffic elsewhere in the city in the evening. This suggests that people dine at neighborhood establishments close to their home in the evening.
```{r}
dat_restaurants <-
  dat_hour %>%
  filter(top_category == "Restaurants and Other Eating Places") 

dat_restaurant_filter <-
  dat_restaurants %>%
  dplyr::rename(., GEOID10 = poi_cbg) %>%
  dplyr::group_by(GEOID10, Hour) %>%
  dplyr::summarize(Avg_Popularity = mean(Count),
            Total_Visits = sum(Count)) %>%
  left_join(phl_cbg) %>% 
  st_as_sf() %>%
  mutate(Visits_Per_Area = Total_Visits / Shape__Area * 100)

#Animation of restaurant popularity by hour
restaurant.animation.data <-
    dat_restaurant_filter %>%
    st_sf() %>%
    mutate(Pop_String = case_when(Visits_Per_Area < .4 ~ "5",
                              Visits_Per_Area >= .4 & Visits_Per_Area <.8 ~ "4",
                              Visits_Per_Area >= .8 & Visits_Per_Area <1.2 ~ "3",
                              Visits_Per_Area >= 1.2 & Visits_Per_Area <1.6 ~ "2",
                              Visits_Per_Area >= 2 ~ "1")) %>%
    mutate(Pop_String  = fct_relevel(Pop_String, "5","4","3","2","1"))

restaurant_animation <-
  ggplot() +
  geom_sf(data = phl_cbg, fill = "#440255", color = "transparent") +   
  geom_sf(data = restaurant.animation.data, aes(fill = Pop_String), color = "transparent") +
    scale_fill_manual(values = palette5) +
    labs(title = "Restaurant Popularity by Hour",
         subtitle = "One Hour Intervals: {current_frame}") +
  theme(panel.background = element_rect(fill = "black"),
         panel.grid.major = element_line(color = "transparent"),
          panel.grid.minor = element_line(colour = "transparent")) +
    transition_manual(Hour)
  

animate(restaurant_animation, duration=20, renderer = gifski_renderer())
```

The following animation shows the same metric for bars. We observe traffic increasing throughout the day starting in the afternoon.
```{r bar animation}
#Bar Analysis
#Filter Bars
dat_bars <- dat_hour %>%
  filter(top_category == "Drinking Places (Alcoholic Beverages)") 

#Merge CBGs with popularity by hour data
dat_bars_filter <-
  dat_bars %>%
  dplyr::rename(., GEOID10 = poi_cbg) %>%
  dplyr::group_by(GEOID10, Hour) %>%
  dplyr::summarize(Avg_Popularity = mean(Count),
            Total_Visits = sum(Count)) %>%
  left_join(phl_cbg) %>% 
  st_as_sf() %>%
  mutate(Visits_Per_Area = Total_Visits / Shape__Area * 100)

#Animation of bar popularity by hour
bar.animation.data <-
    dat_bars_filter %>%
    st_sf() %>%
    mutate(Pop_String = case_when(Visits_Per_Area == 0 ~ "5",
                              Visits_Per_Area > 0 & Visits_Per_Area <.2 ~ "4",
                              Visits_Per_Area >= .2 & Visits_Per_Area <.4 ~ "3",
                              Visits_Per_Area >= .4 & Visits_Per_Area <.6 ~ "2",
                              Visits_Per_Area >= .6 ~ "1")) %>%
    mutate(Pop_String  = fct_relevel(Pop_String, "5","4","3","2","1"))

bar_animation <-
  ggplot() +
  geom_sf(data = phl_cbg, fill = "#440255", color = "transparent", ) +
  geom_sf(data = bar.animation.data, aes(fill = Pop_String), color = "transparent") +
    scale_fill_manual(values = palette5) +
    labs(title = "Bar Popularity by Hour",
         subtitle = "One Hour Intervals: {current_frame}") +
  theme(panel.background = element_rect(fill = "black"),
         panel.grid.major = element_line(color = "transparent"),
          panel.grid.minor = element_line(colour = "transparent")) +
    transition_manual(Hour)

animate(bar_animation, duration=20, renderer = gifski_renderer())
```

## Trip Flows

### Origins & destinations

The SafeGraph data fundamentally captures flow of people across space by connecting a series of origins and destinations. By mapping the origins and destinations of trips taking place across Philadlephia, we can observe which regions of the city draw from a larger crowd across the city, and which areas cater to a more local population.

The following maps look at the origins and destinations for trips to restaurants, bars, and arts venues across the city. Specifically, it shows the distance between the centroid of the origin neighborhood to the centroid of the destination commercial corridor. We selected 6 corridors across Philadelphia in different areas of the city that we felt represented a diverse array of commercial corridors in the city. Going forward, we would like to run a K-means clustering test to help sort the corridors into like categories that will help us understand the distinct profiles of corridors in Philadelphia.

The code draws each line segments based on lat/long coordinates of the origin an destination centroid (could not get this to work with projected data). Note that trips to destinations located outside of the commercial corridors are left off of these maps. 

The following code block loads unprojected shapefiles for the block groups and city boundary as well as wrangle the data into a list of individual origins and destinations for nightlife-related businesses.
```{r flows data wrangling}
flows <- dat2 %>%
  filter(top_category == "Drinking Places (Alcoholic Beverages)" |
           top_category == "Restaurants and Other Eating Places" |
           top_category == "Promoters of Performing Arts, Sports, and Similar Events" |
           top_category == "Performing Arts Companies") %>% #filter for business types
  st_join(phl_corridors) %>% #join destinations to phl corridors (apply corridor destination to each trip)
  st_as_sf() %>%
  st_drop_geometry() %>% 
  select(safegraph_place_id, 
         date_range_start,
         top_category,
         poi_cbg,
         visitor_home_cbgs,
         NAME.y) %>% #select columns
  mutate(visitor_home_cbgs = str_remove_all(visitor_home_cbgs, pattern = "\\[|\\]")) %>%
  mutate(visitor_home_cbgs = str_remove_all(visitor_home_cbgs, pattern = "\\{|\\}")) %>%
  mutate(visitor_home_cbgs = str_remove_all(visitor_home_cbgs, pattern = '\\"|\\"')) %>%
  mutate(visitor_home_cbgs = str_split(visitor_home_cbgs, pattern = ",")) %>%
  unnest(visitor_home_cbgs) %>% #unnest visitor cbg column
  separate(.,
           visitor_home_cbgs,
           c("visitor_cbg", "count"),
           sep = ":") %>% #separate count column from visitor cbg
  mutate(count = as.numeric(count),
         visitor_cbg = as.numeric(visitor_cbg)) %>%
  dplyr::rename(., corridor_dest = NAME.y) %>%
  drop_na(corridor_dest) #Remove trips with destinations outside of corridors

#prepare PHL boundary file to coordinates for plotting
phl_boundary_unproj <- 
  phl_boundary_unproj %>% 
  st_coordinates() # split out coordinates 

phl_boundary_unproj <-
  as.data.frame(phl_boundary_unproj) # save as dataframe

#----MAP V1: NEIGHBORHOOD CENTROID TO CORRIDOR CENTROIDS
#Generate CBG centroid
phl_cbg_cent <- 
  phl_cbg_unproj %>% 
  select(GEOID10, geometry) %>% 
  st_centroid()

#Generate neighborhood centroids
phl_nhood_cent <- 
  phl_nhoods_unproj %>% 
  select(name) %>%
  st_centroid()

#Generate corridor centroid
phl_corr_cent <- 
  phl_corridors_unproj %>% 
  select(NAME, geometry) %>% 
  st_centroid()

#Join CBG centroid to neighborhood shapefile
phl_cbg_nhood <- 
  st_join(phl_nhoods_unproj, phl_cbg_cent, join = st_intersects) %>%
  select(GEOID10, name) %>%
  st_centroid()

flows_nhood <- flows %>%
  left_join(phl_cbg_nhood, by=c("visitor_cbg"="GEOID10")) %>% #join visitor CBGs to nhoods
  dplyr::rename(., nhood_origin = name) %>% #cleanup columns for clarity
  drop_na(nhood_origin) %>% #dropping trips outside of Philadelphia
  st_as_sf() %>%
  st_drop_geometry() %>% 
  group_by(nhood_origin, top_category, corridor_dest) %>% #grouping trip counts by origin neighborhood
  summarize(count = sum(count)) %>%
  left_join(phl_nhood_cent, by=c("nhood_origin"="name")) %>% #join origin nhood to nhood centroid
  left_join(., phl_corr_cent, by=c("corridor_dest"="NAME")) %>% #join destination corridor to corr centroid
  dplyr::rename(., origin.geom = geometry.x,
                        dest.geom = geometry.y) #clean-up columns for clarity

#Convert from tibble to data frame for next step
flows_nhood <- as.data.frame(flows_nhood) 

#split point data into lat and long columns
flows_nhood <- flows_nhood %>% 
  mutate(lat.origin = unlist(map(flows_nhood$origin.geom,1)),
         long.origin = unlist(map(flows_nhood$origin.geom,2)),
         lat.dest = unlist(map(flows_nhood$dest.geom,1)),
         long.dest = unlist(map(flows_nhood$dest.geom,2)),
         id = as.character(c(1:nrow(.))))

#Maps - straight line segment example
flows_nhood %>% 
  filter(corridor_dest == "East Girard" | 
           corridor_dest == 'Navy Yard' | 
           corridor_dest == '5th and Olney' | 
           corridor_dest == 'East Passyunk' |
           corridor_dest == '36th Street and vicinity' |
           corridor_dest == 'Market West') %>% 
  ggplot() + 
  geom_polygon(data = phl_boundary_unproj, aes(x=X, y=Y), fill = "grey20") + 
  geom_segment(aes(x = lat.origin, y = long.origin, xend = lat.dest, yend = long.dest,  
                   color=count, alpha = count),
               arrow = arrow(length = unit(0.01, "cm")), 
               size = 1,
               lineend = "round") +
  scale_colour_distiller(palette="Reds", name="Count", guide = "colorbar", direction = 1) +
  coord_equal() +
  mapTheme() + 
  facet_wrap(~corridor_dest, ncol = 2) +
  labs(title = "Trips")
```

## Nightlife corridors in Philadelphia

Next, we observe foot traffic along Philadelphia's commercial corridors. This analysis relies on a shapefile from the City of Philadelphia's Planning department which demarcates individual corridors and districts throughout the city. According to the [available metadata](https://metadata.phila.gov/#home/datasetdetails/564236a55737e1f263ae5e3f/representationdetails/56423a4e902dbdd813db9a55/) "locations range from large, regional and specialty destinations to corridors that reflect the evolving economy, culture, and aesthetic traditions of surrounding neighborhoods." This means that the gegoraphies vary in size and character.

Figure X.X shows the different types of commercial corridors based on classifcation by Philadelphia.
[INCLUDE DESCRIPTION OF COMMERCIAL CORRIDORS]

```{r}
###Corridors
phl_corridors %>%
  ggplot() +
  geom_sf(data = phl_boundary, fill = "grey60", color = "transparent") +
  geom_sf(aes(fill = corr_type), color = "transparent") +
  scale_fill_viridis_d() +
  labs(title = "Philadelphia Corridor Typologies",
       subtitle = "Figure X.X") +
  mapTheme() +
  theme(legend.position = "bottom",
        legend.title = element_blank())
```

The below Figure maps the commercial corridors and shows the corridor-wide vacancy rate, which varies from close to 0% to upwards of 60% throughout the city. Smaller corridors in far West Philadelphia, North Philadelphia tend to have the highest vacancy rates.
```{r}
#Load Philadelphia commercial corridors dataset and transform
corridors <- phl_corridors

dat_restaurants <-
dat_hour %>%
  filter(top_category == "Restaurants and Other Eating Places") 

corridors_filter <- corridors %>%
  select(OBJECTID, NAME, GLA, P_DIST, ST_EXT, PT_ADD, VAC_RATE) %>%
  mutate(VAC_RATE = str_remove_all(VAC_RATE, pattern = "%")) %>%
  mutate(VAC_RATE = as.numeric(VAC_RATE)) %>%
  st_as_sf() %>%
  st_transform('ESRI:102271') 

# Plot commercial corridors with colors as vacancy rate
ggplot() +
  geom_sf(data = phillyBoundary, fill = "black") +
  geom_sf(data = corridors_filter, aes(fill = VAC_RATE), color = "transparent") +
  scale_fill_viridis_c(option = "plasma") +
  labs(title = "Philadelphia Commercial Corridors") + 
  mapTheme()
```

Figure X.X below breaks down the count of restaunts in a given corridor per square miile.  We find that central corridors and districts tend to have more restaurants per square mile. Other corridors. That said, corridors across the city have a high concentration of restaurants as well.
```{r}
###Restaurants (takes a really long time)
dat_restaurants_grid <- dat_restaurants %>%
  select(geometry) %>%
  na.omit() %>%
  st_as_sf() %>%
  st_transform('ESRI:102271') %>% 
    distinct()

#Merge restaurants with commercial corridors 
corridors_restaurants <- 
  dplyr::select(dat_restaurants_grid) %>% 
  mutate(countRestaurant = 1) %>% 
  aggregate(., corridors_filter, sum) %>%
  mutate(countRestaurant = replace_na(countRestaurant, 0),
         uniqueID = rownames(.),
         area = st_area(geometry) * .00000038610,
         count_per_mile = as.numeric(countRestaurant / area),
         cvID = sample(round(nrow(corridors_filter) / 24), size=nrow(corridors_filter), replace = TRUE))

#Plot map of number of restaurants per square mile
ggplot() +
  geom_sf(data = phillyBoundary, fill = "black") +
  geom_sf(data = corridors_restaurants, aes(fill = count_per_mile), color = "transparent") +
  scale_fill_viridis(trans = "sqrt") +
  labs(title = "Count of Restaurants per Square Mile in Each Commercial Corridor",
       subtitle = "Figure X.X") +
  mapTheme()
```

Below is an animation showing restaurant popularity by corridor.
```{r}
#Create animated map of number of restaurant trips by corridor per hour
dat_restaurants_cord <- dat_restaurants %>%
  st_as_sf() %>%
  st_transform('ESRI:102271') 

dat_corridors_restaurants <-
  st_join(corridors_filter, dat_restaurants_cord, ) %>% 
  group_by(NAME, GLA, Hour) %>%
  summarize(
            Avg_Popularity = mean(Count),
            Total_Visits = sum(Count),
            Med_Dwell_Time = mean(median_dwell)) %>%
  st_as_sf() 

dat_corridors_restaurants <- dat_corridors_restaurants %>%
  mutate(
    GLA = as.numeric(gsub(",", "", GLA)),
    Visits_Per_Area = Total_Visits / GLA * 5)

#Plot of restaurant popularity between 7 and 8pm
# dat_corridors_restaurants %>%
#   subset(Hour == 19) %>% #selecting for nighttime hours
#   ggplot() + 
#   geom_sf(data = phl_cbg, fill = "#440255", color = "transparent") +
#   geom_sf(aes(fill = Visits_Per_Area), color = "transparent") + 
#   scale_fill_viridis(trans = "sqrt") +
#   labs(title = "Total Restaurant Visits by Commercial Corridor") 


#Animation of restaurant popularity by hour
restaurant.corr.animation.data <-
    dat_corridors_restaurants %>%
    st_sf() %>%
    mutate(Pop_String = case_when(Visits_Per_Area < .4 ~ "5",
                              Visits_Per_Area >= .4 & Visits_Per_Area <.8 ~ "4",
                              Visits_Per_Area >= .8 & Visits_Per_Area <1.2 ~ "3",
                              Visits_Per_Area >= 1.2 & Visits_Per_Area <1.6 ~ "2",
                              Visits_Per_Area >= 2 ~ "1")) %>%
    mutate(Pop_String  = fct_relevel(Pop_String, "5","4","3","2","1"))

restaurant_corr_animation <-
  ggplot() +
  geom_sf(data = phl_cbg, fill = "#440255", color = "transparent") +   
  geom_sf(data = restaurant.corr.animation.data, aes(fill = Visits_Per_Area), color = "transparent") +
     scale_fill_viridis(trans = "sqrt") +
    labs(title = "Restaurant Popularity by Hour",
         subtitle = "One Hour Intervals: {current_frame}") +
  theme(panel.background = element_rect(fill = "black"),
         panel.grid.major = element_line(color = "transparent"),
          panel.grid.minor = element_line(colour = "transparent")) +
    transition_manual(Hour)

animate(restaurant_corr_animation, duration=20, renderer = gifski_renderer())
```

Figure X.X shows the number of bars in each commercial corridor per square mile. Again, we see the corridors around Center City generally showing a higher concentration of bars.
```{r Bars by corridor}
###Bars
dat_bars_grid <- dat_bars %>%
  select(geometry) %>%
  #na.omit() %>%
  st_as_sf() %>%
  st_transform('ESRI:102271') %>%
    distinct()

###Bars
corridors_bars <- 
  dplyr::select(dat_bars_grid) %>% 
  mutate(countBar = 1) %>% 
  aggregate(., corridors_filter, sum) %>%
  mutate(countBar = replace_na(countBar, 0),
         uniqueID = rownames(.),
         area = st_area(geometry) * .00000038610,
         count_per_mile = as.numeric(countBar / area),
         cvID = sample(round(nrow(corridors_filter) / 24), size=nrow(corridors_filter), replace = TRUE))

#Plot map of number of bars per square mile
ggplot() +
  geom_sf(data = phillyBoundary, fill = "black") +
  geom_sf(data = corridors_bars, aes(fill = count_per_mile), color = "transparent") +
  scale_fill_viridis(trans = "sqrt") +
  labs(title = "Count of Bars per Square Mile in Each Commercial Corridor", 
       subtitle = "Figure X.X") +
  mapTheme()
```

Below is an animation showing bar popularity aggregated by commercial corridor.
```{r bars animation}
#Create animated map of number of bar trips by corridor per hour
dat_bars_cord <- dat_bars %>%
  st_as_sf() %>%
  st_transform('ESRI:102271') 

dat_corridors_bars <-
  st_join(corridors_filter, dat_bars_cord, ) %>% 
  group_by(NAME, GLA, Hour) %>%
  summarize(Avg_Popularity = mean(Count),
            Total_Visits = sum(Count),
            Med_Dwell_Time = mean(median_dwell)) %>%
  st_as_sf() 

dat_corridors_bars <- dat_corridors_bars %>%
  mutate(
    GLA = as.numeric(gsub(",", "", GLA)),
    Visits_Per_Area = Total_Visits / GLA * 5)

#Plot of bar popularity between 7 and 8pm
# dat_corridors_bars %>%
#   subset(Hour == 19) %>% #selecting for nighttime hours
#   ggplot() + 
#   geom_sf(data = phl_cbg, fill = "#440255", color = "transparent") +
#   geom_sf(aes(fill = Visits_Per_Area), color = "transparent") + 
#   scale_fill_viridis(trans = "sqrt") +
#   labs(title = "Total Bar Visits by Commercial Corridor") 

#Animation of bar popularity by hour
bar.corr.animation.data <-
    dat_corridors_bars %>%
    st_sf() %>%
    mutate(Pop_String = case_when(Visits_Per_Area < .4 ~ "5",
                              Visits_Per_Area >= .4 & Visits_Per_Area <.8 ~ "4",
                              Visits_Per_Area >= .8 & Visits_Per_Area <1.2 ~ "3",
                              Visits_Per_Area >= 1.2 & Visits_Per_Area <1.6 ~ "2",
                              Visits_Per_Area >= 2 ~ "1")) %>%
    mutate(Pop_String  = fct_relevel(Pop_String, "5","4","3","2","1"))

bar_corr_animation <-
  ggplot() +
  geom_sf(data = phl_cbg, fill = "#440255", color = "transparent") +   
  geom_sf(data = bar.corr.animation.data, aes(fill = Visits_Per_Area), color = "transparent") +
     scale_fill_viridis(trans = "sqrt") +
    labs(title = "Bar Popularity by Hour",
         subtitle = "One Hour Intervals: {current_frame}") +
  theme(panel.background = element_rect(fill = "black"),
         panel.grid.major = element_line(color = "transparent"),
          panel.grid.minor = element_line(colour = "transparent")) +
    transition_manual(Hour)

animate(bar_corr_animation, duration=20, renderer = gifski_renderer())
```

Next, we look at SafeGraph variables in the context of commercial corridor typology. The following code wrangles the data to split it out into time segments.  This will help us better understand which corridor types are most associated with nightlife.
```{r}
dat_hour_unnest <- 
  dat2 %>% 
  select(safegraph_place_id, date_range_start, popularity_by_hour) %>%
  mutate(popularity_by_hour = str_remove_all(popularity_by_hour, pattern = "\\[|\\]")) %>%
  unnest(popularity_by_hour) %>%
  separate(.,
           popularity_by_hour,
           c("0", "1", "2", "3", "4", "5", "6", 
             "7", "8", "9", "10", "11", "12", 
             "13", "14", "15", "16", "17", "18",
             "19", "20", "21", "22", "23"),
           sep = ",") %>%
  pivot_longer(cols = 3:26,
               names_to = "Hour",
               values_to = "Count") %>%
  mutate(Hour = as.numeric(Hour),
         Count = as.numeric(Count))

# dat2 %>% 
#   select(safegraph_place_id, date_range_start, popularity_by_hour) %>%
#   mutate(popularity_by_hour = str_remove_all(popularity_by_hour, pattern = "\\[|\\]")) %>%
#   unnest(popularity_by_hour) %>%
#   separate(.,
#            popularity_by_hour,
#            c("0", "1", "2", "3", "4", "5", "6", 
#              "7", "8", "9", "10", "11", "12", 
#              "13", "14", "15", "16", "17", "18",
#              "19", "20", "21", "22", "23"),
#            sep = ",") %>%
#   pivot_longer(cols = 3:26,
#                names_to = "Hour",
#                values_to = "Count") %>%
#   mutate(Hour = as.numeric(Hour),
#          Count = as.numeric(Count))

dat_1_6 <- dat_hour_unnest %>%
  filter(Hour == "1" | 
           Hour == "2" | 
           Hour == "3" | 
           Hour == "4" | 
           Hour == "5" |
           Hour == "6") %>%
  group_by(safegraph_place_id, date_range_start) %>%
  summarize(Hrs1_6 = sum(Count)) 
dat_7_12 <- dat_hour_unnest %>%
  filter(Hour == "7" | 
           Hour == "8" | 
           Hour == "9" | 
           Hour == "10" | 
           Hour == "11" |
           Hour == "12") %>%
  group_by(safegraph_place_id, date_range_start) %>%
  summarize(Hrs7_12 = sum(Count)) 
dat_13_18 <- dat_hour_unnest %>%
  filter(Hour == "13" | 
           Hour == "14" | 
           Hour == "15" | 
           Hour == "16" | 
           Hour == "17" |
           Hour == "18") %>%
  group_by(safegraph_place_id, date_range_start) %>%
  summarize(Hrs13_18 = sum(Count)) 
dat_19_0 <- dat_hour_unnest %>%
  filter(Hour == "19" | 
           Hour == "20" | 
           Hour == "21" | 
           Hour == "22" | 
           Hour == "23" | 
           Hour == "0") %>%
  group_by(safegraph_place_id, date_range_start) %>%
  summarize(Hrs19_0 = sum(Count)) 
dat_workday <- dat_hour_unnest %>%
  filter(Hour == "9" | 
           Hour == "10" | 
           Hour == "11" | 
           Hour == "12" | 
           Hour == "13" | 
           Hour == "14" |
           Hour == "15" |
           Hour == "16" |
           Hour == "17" |
           Hour == "18") %>%
  group_by(safegraph_place_id, date_range_start) %>%
  summarize(Hrs_workday = sum(Count))

#Join new dataset
dat3 <- 
  dat2 %>% 
  left_join(dat_1_6, by = c('safegraph_place_id', 'date_range_start')) %>%
  left_join(dat_7_12, by = c('safegraph_place_id', 'date_range_start')) %>%
  left_join(dat_13_18, by = c('safegraph_place_id', 'date_range_start')) %>%
  left_join(dat_19_0, by = c('safegraph_place_id', 'date_range_start')) %>%
  left_join(dat_workday, by = c('safegraph_place_id', 'date_range_start')) %>%
  st_join(phl_corridors, join = st_intersects) %>%
  st_join(phl_nhoods, join = st_intersects) %>%
  mutate(WorkDay_Evening_Ratio =  Hrs_workday / Hrs19_0)
```

Figure X.X below shows the total daily traffic volume by time segment. While there is not drastic differentiation by corridor, this chart indicates that Regional and Superregional corridors have a larger relative share of daytime traffic. This is likely related to these corridors being job hubs in Phialdelphia.
```{r popularity by day barplot}
dat3 %>%
  st_drop_geometry() %>%
  drop_na(corr_type) %>%
  group_by(corr_type) %>%
  summarise(Early_AM = sum(Hrs1_6),
            Late_AM = sum(Hrs7_12),
            Early_PM = sum(Hrs13_18),
            Late_PM = sum(Hrs19_0)) %>%
  pivot_longer(cols = 2:5,
               names_to = "Time",
               values_to = "Traffic") %>%
  ggplot(aes(fill=factor(Time, levels=c("Late_PM", 
                                        "Early_PM", 
                                        "Late_AM", 
                                        "Early_AM")), 
             y=Traffic, 
             x=factor(corr_type, levels=c("6. Speciality Center",
                                          "5. Superregional Center",
                                          "4. Regional Center",
                                          "3. Community Center",
                                          "2. Neighborhood Center",
                                          "1. Neighborhood Subcenter")))) + 
  geom_bar(position="fill", stat="identity") +
  coord_flip() +
  scale_fill_viridis_d() +
  labs(title = "Total Daily Traffic Volume by Corridor Type",
       subtitle = "Figure X.X") +
  guides(fill=guide_legend(title=NULL)) +
  plotTheme() +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  guides(fill = guide_legend(reverse = TRUE))

```

Figure X.X calculates the ratio between daytime traffic between 9AM and 6PM and evening traffic between 7PM and 12AM. The higher the ratio, the more daytime a corridor type is. Similar to the above Figure, This indicates that Regional and Superregional traffic sees relatively more traffic during work hours.
```{r}
#Workday to Evening Traffic Ratio by Corridor Type
dat3 %>%
  st_drop_geometry() %>%
  drop_na(corr_type) %>%
  filter_at(vars(WorkDay_Evening_Ratio), all_vars(!is.infinite(.))) %>%
  group_by(corr_type) %>%
  summarise(WorkDay_Evening_Ratio = mean(WorkDay_Evening_Ratio, na.rm = TRUE)) %>%
  ggplot(aes(y=WorkDay_Evening_Ratio, 
             x=corr_type)) + 
  geom_bar(stat="identity") +
  scale_fill_viridis_d() +
  labs(title = "Average Workday to Evening Traffic Ratio by Corridor Type",
       subtitle = "Figure X.X") +
  guides(fill=guide_legend(title=NULL)) +
  plotTheme()
```

## Visit duration to bars and restaurants in Philadelphia

The following Figures look at the median dwell time by bars and restaurants per commercial corridor.
```{r}
#Restaurant data
dat_corridors_restaurants <-
  st_join(corridors_filter, dat_restaurants_cord, ) %>% 
  group_by(NAME, GLA, Hour) %>%
  summarize(
            Avg_Popularity = mean(Count),
            Total_Visits = sum(Count),
            Med_Dwell_Time = mean(median_dwell)) %>%
  st_as_sf() 

dat_corridors_restaurants <- dat_corridors_restaurants %>%
  mutate(
    GLA = as.numeric(gsub(",", "", GLA)),
    Visits_Per_Area = Total_Visits / GLA * 5)

#Plot of restaurant median dwell time 
dat_corridors_restaurants %>%
  subset(Hour == 19) %>% #selecting for nighttime hours
  ggplot() + 
  geom_sf(data = phl_cbg, fill = "light gray", color = "white") +
  geom_sf(aes(fill = Med_Dwell_Time), color = "transparent") + 
  scale_fill_viridis(trans = "sqrt") +
  labs(title = "Median Restaurant Dwell Time by Commercial Corridor",
       subtitle = "Figure X.X") +
  mapTheme()

#Plot of bar median dwell time 
dat_corridors_bars %>%
  subset(Hour == 19) %>% #selecting for nighttime hours
  ggplot() + 
  geom_sf(data = phl_cbg, fill = "light gray", color = "white") +
  geom_sf(aes(fill = Med_Dwell_Time), color = "transparent") + 
  scale_fill_viridis(trans = "sqrt") +
  labs(title = "Median Bar Dwell Time by Commercial Corridor",
       subtitle = "Figure X.X") +
  mapTheme()
```

## The Impact of COVID-19 on Philadelphia businesses

Now that we have an overview of the nightlife landscape in Philadelphia, we want to explore how business traffic changed during the COVID-19 pandemic. The pandemic has been one of the main impetuses behind our work. The pandemic has wrought economic devastation across a host of Philadelphia businesses, but has particularly impacted the nightlife businesses, which rely on gathering large groups of people together, often indoors. 

The following dataset wrangles data from 2020 and combines it with our base dataset from 2018.
```{r}
##########################
# DATA WRANGLING FOR COVID
##########################
data2020 <- read.csv("./data/moves_monthly2020.csv")

# Modify 2018 data
dat_2018 <- 
  dat %>% 
  mutate(popularity_by_hour = str_remove_all(popularity_by_hour, pattern = "\\[|\\]")) %>% #remove brackets
  unnest(popularity_by_hour) %>% #unnest values
  separate(.,
           popularity_by_hour,
           c("18",
             "19", "20", "21", "22", "23"),
           sep = ",") %>%
  mutate(.,NightVisits2018 = as.numeric(`18`) + as.numeric(`19`) + as.numeric(`20`) + as.numeric(`21`) + as.numeric(`22`) + as.numeric(`23`))

dat_2018 <- dat_2018 %>% 
  dplyr::select(safegraph_place_id, 
                location_name,
                date_range_start, 
                date_range_end, 
                raw_visit_counts,
                raw_visitor_counts, 
                #visits_by_day, 
                #poi_cbg, 
                #visitor_home_cbgs, 
                #visitor_daytime_cbgs, 
                #visitor_work_cbgs, 
                #visitor_country_of_origin,
                #distance_from_home, 
                #median_dwell, 
                #bucketed_dwell_times, 
                #related_same_day_brand, 
                #related_same_month_brand, 
                #popularity_by_hour, 
                #device_type,
                popularity_by_day,
                NightVisits2018) %>%
  rename(raw_visit_counts2018 = raw_visit_counts, raw_visitor_counts2018 = raw_visitor_counts, popularity_by_day2018 = popularity_by_day) %>%
  mutate(month = substring(date_range_start,6,7))

dat_2020 <- 
  data2020 %>% 
  mutate(popularity_by_hour = str_remove_all(popularity_by_hour, pattern = "\\[|\\]")) %>% #remove brackets
  unnest(popularity_by_hour) %>% #unnest values
  separate(.,
           popularity_by_hour,
           c("18",
             "19", "20", "21", "22", "23"),
           sep = ",") %>%
  mutate(.,NightVisits2020 = as.numeric(`18`) + as.numeric(`19`) + as.numeric(`20`) + as.numeric(`21`) + as.numeric(`22`) + as.numeric(`23`))

dat_join <- dat_2020 %>% 
  dplyr::select(safegraph_place_id, 
                location_name,
                date_range_start, 
                date_range_end, 
                raw_visit_counts,
                raw_visitor_counts, 
                #visits_by_day, 
                #poi_cbg, 
                #visitor_home_cbgs, 
                #visitor_daytime_cbgs, 
                #visitor_work_cbgs, 
                #visitor_country_of_origin,
                #distance_from_home, 
                #median_dwell, 
                #bucketed_dwell_times, 
                #related_same_day_brand, 
                #related_same_month_brand, 
                #popularity_by_hour, 
                #device_type,
                popularity_by_day,
                NightVisits2020) %>%
  rename(raw_visit_counts2020 = raw_visit_counts, 
         raw_visitor_counts2020 = raw_visitor_counts, 
         popularity_by_day2020 = popularity_by_day) %>%
  mutate(month = substring(date_range_start,6,7)) %>%
  inner_join(., dat_2018, by = c("safegraph_place_id", "month", "location_name")) %>%
  mutate(Month = case_when(month == "01" ~ "January",
                    month == "02" ~ "February",
                    month == "03" ~ "March",
                    month == "04" ~ "April",
                    month == "05" ~ "May", 
                    month == "06" ~ "Jun",
                    month == "07" ~ "May",
                    month == "08" ~ "May",
                    month == "09" ~ "May",
                    month == "10" ~ "May",
                    month == "11" ~ "May",
                    month == "12" ~ "May"))

dat_join2 <- dat_join %>% 
  dplyr::select(safegraph_place_id, 
                location_name,
                raw_visit_counts2018,
                raw_visit_counts2020,
                NightVisits2018,
                NightVisits2020,
                popularity_by_day2018,
                popularity_by_day2020,
                month) %>%
  left_join(., phila, by = "safegraph_place_id") %>% 
  st_as_sf() %>%
  st_transform('ESRI:102728') 
```

First, we observe citywide changes in foot traffic over the course of the pandemic. Comparing 2020 traffic to our 2018 dataset, we see that traffic across the city drastically decreased in March 2020 and represented less then half of the 2018 traffic. This trend continued thorughout the year.
```{r}
#Citywide nighttime visits
dat_citywide <- dat_join2 %>%
  filter(top_category == "Drinking Places (Alcoholic Beverages)" |
           top_category == "Restaurants and Other Eating Places" |
           top_category == "Promoters of Performing Arts, Sports, and Similar Events" |
           top_category == "Performing Arts Companies") %>%
  group_by(month) %>%
  summarize(Total_Visits2018 = sum(NightVisits2018),
            Total_Visits2020 = sum(NightVisits2020)) %>%
  mutate(Percent_Change = (Total_Visits2020 - Total_Visits2018)/Total_Visits2018*100)

#Plot citywide chart
dat_citywide %>%
ggplot(., aes(x = month, y = Percent_Change, fill = Percent_Change)) + 
  geom_col() +
  scale_fill_distiller(palette="PiYG", direction = 1) +
  labs(title = "Philadelphia Commercial Trip % Change, 2018 & 2020",
       subtitle = "Figure X.X", y = "Percent Change", x = "Month") +
  plotTheme() 
```

Next we split the percent change in traffic out by business types relevant to our nightlife study: arts establishments, bars, and restaurants. While arts establishments indicate that there was some recovery towards the end of 2020, bars and restaurants suffered decreased traffic throughout the year. This divergent trend from the arts establishments could be related to stricter policies regarding occupancy at bars and restaurants, where you have to remove your mask to eat and drink, or public discomfort with visitng these businesses.
```{r}
#Separate by commercial use  
dat_citywide2 <- dat_join2 %>%
    filter(top_category == "Drinking Places (Alcoholic Beverages)" |
             top_category == "Restaurants and Other Eating Places" |
             top_category == "Promoters of Performing Arts, Sports, and Similar Events" |
             top_category == "Performing Arts Companies") %>%
  mutate(category = case_when(top_category == "Drinking Places (Alcoholic Beverages)" ~ "Bars",
           top_category == "Restaurants and Other Eating Places" ~ "Restaurants",
           top_category == "Promoters of Performing Arts, Sports, and Similar Events" ~ "Arts",
           top_category == "Performing Arts Companies" ~ "Arts")) %>%
    group_by(category, month) %>%
    summarize(Total_Visits2018 = sum(NightVisits2018),
              Total_Visits2020 = sum(NightVisits2020)) %>%
    mutate(Percent_Change = (Total_Visits2020 - Total_Visits2018)/Total_Visits2018*100)  
  
#Plot citywide chart by commercial use
dat_citywide2 %>%
  ggplot(., aes(x = month, y = Percent_Change, fill = Percent_Change)) + 
  geom_col() +
  labs(title = "Philadelphia Commercial Trip % Change, 2018 & 2020",
       subtitle = "Figure X.X") +
  scale_fill_distiller(palette="PiYG", direction = 1) +
  facet_wrap(~category) +
  plotTheme()
```

Lookign at the trends another way, the below line plot shows how the traffic patterns for bars and restaurants were very similar, but the arts establishments made a stronger rebound at the end of 2020.
```{r}
dat_citywide2 %>%
  ggplot(., aes(x = month, y = Percent_Change, group = category, color = category)) + 
  geom_line(lwd = 1.5) +
  geom_hline(yintercept=0, lwd = 1.5, linetype="dotted")+
  labs(title = "Philadelphia Commercial Trip % Change, 2018 & 2020",
       subtitle = "Figure X.X") +
  plotTheme()
```

The following figure shows the change in restaurant traffic by Philadelphia block group. With the exception of a few areas, all block groups indicated a sharp decrease in restaurant traffic, many by over 50%.
```{r}
#Filter by Restaurants
dat_restaurants <- dat_join2 %>%
  filter(top_category == "Restaurants and Other Eating Places") %>%
  filter(month == '04'| month == '05' | month == '06' | month == '07' | month == '08' | month == '09' | month =='10' | month == '11' | month == '12') %>%
  mutate(GEOID10 = as.numeric(GEOID)) %>%
  group_by(GEOID10) %>%
  summarize(Total_Visits2018 = sum(NightVisits2018),
            Total_Visits2020 = sum(NightVisits2020)) %>%
  st_drop_geometry() %>%
  left_join(phl_cbg) %>% 
  st_as_sf() %>%
  mutate(Percent_Change = (Total_Visits2020 - Total_Visits2018)/Total_Visits2018*100)

#Plot restaurant map
dat_restaurants %>%  
 # filter(Percent_Change < 300) %>%
  mutate(percent_change = case_when(Percent_Change > 100 ~ 100, Percent_Change <= 100 ~ Percent_Change)) %>%
  ggplot() + 
  geom_sf(data = phl_boundary, fill = "grey20", color = "black")+
  geom_sf(aes(fill = percent_change), color = "transparent") + 
  scale_fill_distiller(palette="RdYlGn", direction = 1) +
  labs(title = "Restaurant Trip % Change, April 2018 & April 2020") +
  mapTheme()
```

The same is largely true of bars, thoguh there are some exceptions. It is possible that some of the areas showing an increase in traffic were related to the availability of outdoor space, as more densly populated areas of the city underwent a severe drop.
```{r}
#Filter by Bars
dat_bars <- dat_join2 %>%
  filter(top_category == "Drinking Places (Alcoholic Beverages)") %>%
  filter(month == '04'| month == '05' | month == '06' | month == '07' | month == '08' | month == '09' | month =='10' | month == '11' | month == '12') %>%
  mutate(GEOID10 = as.numeric(GEOID)) %>%
  group_by(GEOID10) %>%
  summarize(Total_Visits2018 = sum(NightVisits2018),
            Total_Visits2020 = sum(NightVisits2020)) %>%
  st_drop_geometry() %>%
  left_join(phl_cbg) %>% 
  st_as_sf() %>%
  mutate(Percent_Change = (Total_Visits2020 - Total_Visits2018)/Total_Visits2018*100)

#Plot bar map
dat_bars %>%  
  # filter(Percent_Change < 300) %>%
  mutate(percent_change = case_when(Percent_Change > 100 ~ 100, Percent_Change <= 100 ~ Percent_Change)) %>%
  ggplot() + 
  geom_sf(data = phl_boundary, fill = "grey20", color = "black")+
  geom_sf(aes(fill = percent_change), color = "transparent") + 
  scale_fill_distiller(palette="RdYlGn", direction = 1) +
  labs(title = "Bar Trip % Change, April 2018 & April 2020") +
  mapTheme()
```

Finally, in line with our earlier analysis on commercial corridors across Philadelphia, this map shows decrease of traffic to nightlife establishments along such corridors. While some smaller corridors did not experience much change in traffic or even a small increase in traffic, the majority of corridors saw a large decrease in traffic. This is particularly evident in Center City, along Market Street in West Philadelphia and at the airport. 
```{r corridor analysis}
corridors_filter <- phl_corridors %>%
  select(OBJECTID, NAME, GLA, P_DIST, ST_EXT, PT_ADD, VAC_RATE) %>%
  mutate(VAC_RATE = str_remove_all(VAC_RATE, pattern = "%")) %>%
  mutate(VAC_RATE = as.numeric(VAC_RATE)) %>%
  st_as_sf() %>%
  st_transform('ESRI:102728') 
  
dat_corr <- dat_join2 %>%
  filter(top_category == "Drinking Places (Alcoholic Beverages)" |
           top_category == "Restaurants and Other Eating Places" |
           top_category == "Promoters of Performing Arts, Sports, and Similar Events" |
           top_category == "Performing Arts Companies") %>%
  filter(month == '04'| month == '05' | month == '06' | month == '07' | month == '08' | month == '09' | month =='10' | month == '11' | month == '12') %>%
  mutate(GEOID10 = as.numeric(GEOID)) %>%
  st_join(corridors_filter) %>% 
  group_by(NAME.y, month) %>%
  summarize(Total_Visits2018 = sum(NightVisits2018),
            Total_Visits2020 = sum(NightVisits2020)) %>%
  mutate(Percent_Change = (Total_Visits2020 - Total_Visits2018)/Total_Visits2018*100) %>%
  drop_na(NAME.y) %>%
  st_as_sf() %>%
  st_transform('ESRI:102728') %>%
  st_drop_geometry() 

dat_corr <- dat_corr %>%
  mutate(NAME = NAME.y) %>%
  left_join(corridors_filter)

#Average across all months
dat_corr_avg <- dat_corr %>%  
  group_by(NAME) %>%
  summarize(Percent_Change = mean(Percent_Change))

###Corridor Visualizations 
#All Commercial Uses Corridor Map
dat_corr %>%  
  filter(month == "05") %>%
  mutate(Percent_Change = case_when(Percent_Change > 100 ~ 100, 
                                    Percent_Change <= 100 ~ Percent_Change)) %>%
  ggplot() + 
  geom_sf(data = phl_boundary, fill = "grey10", color = "darkgrey")+
  geom_sf(aes(fill = Percent_Change, geometry = geometry), color = "transparent") + 
  scale_fill_distiller(palette="RdYlGn", direction = 1) +
  labs(title = "Commercial Nighttime Trip % Change, April 2018 & 2020") +
  mapTheme()

# #% Commercial Trip Drop by Corridor
# dat_corr_avg %>%
#   ggplot(., aes(x = reorder(NAME,-Percent_Change), y = Percent_Change, fill = Percent_Change)) +
#   geom_col() +
#   scale_fill_distiller(palette="PiYG", direction = 1) +
#   labs(title = "Commercial Nighttime Trip % Change April 2018-2020", x = "Commercial Corridors", y = "Percentage Change") +
#   plotTheme() 
```

# 4. Prediction Model

Our model will predict the number of nighttime trips to commercial corridors (trips between the hours of 7PM and 12AM) given features of the corridor such as retail mix, amenities, location, etc. Ultimately, we plan to use this model as a scenario-based tool that allows users to adjust the business mix for a given corriodr as a way to understand how different economic development strategies will impact popularity.

While the model will predict the number of trips, the ultimate output for the user will be relative popularity of the corridor compared to the average. This is becasue the data is inherently noisy and we cannot be confident that a raw trip count is accurate.

## Data Pull
This first code block loads in data that we will use for the predictive model.
```{r loading in prediction data, message=FALSE, warning=FALSE}
#SafeGraph Features
bar.sf <- dat2 %>%
  filter(top_category == "Drinking Places (Alcoholic Beverages)") %>%
  select(location_name) %>%
  st_as_sf(coords = "geometry", crs = 4326, agr = "constant")

restaurant.sf <- dat2 %>%
  filter(top_category == "Restaurants and Other Eating Places") %>%
  select(location_name) %>%
  st_as_sf()

arts.sf <- dat2 %>%
  filter(top_category == "Promoters of Performing Arts, Sports, and Similar Events" |
           top_category == "Performing Arts Companies") %>%
  select(location_name) %>%
  st_as_sf()

college.sf <- dat2 %>%
  filter(top_category == "Colleges, Universities, and Professional Schools") %>%
  select(location_name) %>%
  st_as_sf()

sports.sf <- dat2 %>%
  filter(top_category == "Spectator Sports") %>%
  select(location_name) %>%
  st_as_sf()

casinos.sf <- dat2 %>%
  filter(sub_category == "Casino Hotels") %>%
  select(location_name) %>%
  st_as_sf()

hotels.sf <- dat2 %>%
  filter(sub_category == "Hotels (except Casino Hotels) and Motels") %>%
  select(location_name) %>%
  st_as_sf()

parking.sf <- dat2 %>%
  filter(sub_category == "Parking Lots and Garages") %>%
  select(location_name) %>%
  st_as_sf()

#Philadelphia Features
phl_corridors_pred <- phl_corridors %>%
  st_as_sf() %>%
  dplyr::select(4:5,18, 'corr_type') %>%
  rename(., corridor = NAME,
         district = P_DIST,
         vacancy_rate = VAC_RATE) %>%
  mutate(vacancy_rate = str_remove_all(vacancy_rate, pattern = "%"),
         vacancy_rate = str_remove_all(vacancy_rate, pattern = "\r\n"),
         vacancy_rate = as.numeric(vacancy_rate),
         corr_area_sqft = as.numeric(st_area(phl_corridors)),
         corr_area_sqmi = as.numeric(st_area(phl_corridors)*0.000000035870))

phl_corridors_pred$vacancy_rate <- phl_corridors_pred$vacancy_rate %>% replace_na(0)

phl_dist <-
  st_read("http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson",
          quiet = TRUE) %>%
  st_transform('ESRI:102728')

phl_nhoods <- 
  st_read("https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson", quiet = TRUE) %>%
  st_transform('ESRI:102728') %>%
  select(mapname) %>%
  rename(., neighborhood = mapname)

CenterCity.sf <- phl_dist %>% 
  select(DIST_NAME) %>%
  filter(DIST_NAME == "Central") %>%
  st_centroid()

CityHall.sf <- 
  dat3 %>% 
  select(location_name, date_range_start, top_category) %>%
  filter(location_name == "City Hall", date_range_start == "2018-01-01T05:00:00Z")

Temple.sf <- 
  dat3 %>% 
  select(location_name, date_range_start, top_category) %>%
  filter(location_name == "Temple University", 
         top_category == "Colleges, Universities, and Professional Schools",
         date_range_start == "2018-01-01T05:00:00Z")

UPenn.sf <- 
  dat3 %>% 
  select(location_name, date_range_start, top_category) %>%
  filter(location_name == "Univ of Penn",
         top_category == "Colleges, Universities, and Professional Schools",
         date_range_start == "2018-01-01T05:00:00Z")

septaStops <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/8c6e2575c8ad46eb887e6bb35825e1a6_0.geojson", quiet = TRUE) %>% 
      mutate(Line = "El") %>% 
      st_transform('ESRI:102728') %>%
      select(Station, Line),
    st_read("https://opendata.arcgis.com/datasets/2e9037fd5bef406488ffe5bb67d21312_0.geojson", quiet = TRUE) %>%
      mutate(Line ="Broad_St") %>%
      st_transform('ESRI:102728') %>%
      select(Station, Line)) 

parks <-
  st_read("https://opendata.arcgis.com/datasets/d52445160ab14380a673e5849203eb64_0.geojson", 
          quiet = TRUE) %>%
  st_transform('ESRI:102728') %>%
  select(PUBLIC_NAME)

phl_building_footprints <-
  st_read("https://opendata.arcgis.com/datasets/ab9e89e1273f445bb265846c90b38a96_0.geojson", 
          quiet = TRUE) %>%
  dplyr::select(ADDRESS) %>%
  st_transform('ESRI:102728')

phl_building_footprints <- phl_building_footprints %>%
  mutate(bld_area_sqft = as.numeric(st_area(phl_building_footprints)),
         bld_area_sqmi = as.numeric(st_area(phl_building_footprints)*0.000000035870))

phl_busstop <-
  st_read("https://opendata.arcgis.com/datasets/e09e9f98bdf04eada214d2217f3adbf1_0.geojson") %>%
  st_transform('ESRI:102728') %>%
  .[phl_boundary,] %>%
  filter(Mode == "Bus")

phl_trolley <-
  st_read("https://opendata.arcgis.com/datasets/e09e9f98bdf04eada214d2217f3adbf1_0.geojson") %>%
  st_transform('ESRI:102728') %>%
  .[phl_boundary,] %>%
  filter(Mode == "Trolley")

#Demographic data
phl_blockgroups <- 
  get_acs(geography = "block group", 
          variables = c("B01003_001E", 
                        "B02001_002E", 
                        "B01002_001E",
                        "B19013_001E", 
                        "B25064_001E",
                        "B03002_012E",
                        "B02001_003E"),
          year=2018, 
          state=42, 
          county=101, 
          geometry=T, 
          output = "wide") %>%
  st_transform('ESRI:102728')%>%
  rename(TotalPop = B01003_001E,
         White = B02001_002E,
         MedAge = B01002_001E,
         MedHHInc = B19013_001E,
         MedRent = B25064_001E,
         Hisp = B03002_012E,
         Black = 	B02001_003E,
         GEOID10 = GEOID) %>%
  dplyr::select(-ends_with("M")) %>%
  mutate(pctWhite = ((White / TotalPop)*100),
         pctBlack = ((Black / TotalPop)*100),
         pctHisp = ((Hisp / TotalPop)*100),
         tract_area_mi = as.numeric(st_area(geometry)*0.000000035870),
         popdens_mi = TotalPop / tract_area_mi
         # Context_Race = ifelse(pctWhite > .5, "Majority White", "Majority Non-White"),
         # Context_Income = ifelse(MedHHInc > 46116, "High Income", "Low Income"),
         # Context_Age = ifelse(MedAge > 34.5, "High Median Age", "Low Median Age"),
         # Context_Rent = ifelse(MedRent > 1032, "High Median Rent", "Low Median Rent")
         ) %>%
  st_as_sf()

st_c <- st_coordinates
```

## Feature Engineering
Because we are looking to predict nighttime trips, we must isolate the trips that occur during the evening hours as our depedent variable. The folowing code aggregates datasets created above into a new dataframe that includes Philadelphia information, buildings and data by hourly trips. The dataframe drops all trips with destiantions outside of commercial corridors.
```{r}
dat_pred_temp <- 
  dat2 %>% 
  left_join(dat_1_6, by = c('safegraph_place_id', 'date_range_start')) %>%
  left_join(dat_7_12, by = c('safegraph_place_id', 'date_range_start')) %>%
  left_join(dat_13_18, by = c('safegraph_place_id', 'date_range_start')) %>%
  left_join(dat_19_0, by = c('safegraph_place_id', 'date_range_start')) %>%
  left_join(dat_workday, by = c('safegraph_place_id', 'date_range_start')) %>%
  st_join(phl_blockgroups) %>%
  st_join(phl_nhoods) %>%
  st_join(phl_corridors_pred) %>%
  st_join(., phl_building_footprints) %>%
  drop_na(corridor)
```

Our features fall broadly into the following categories:

* *Philadelphia summary Features*: These are features that capture the built environment, such as the averaage size of the buildings along the corridor and the area of the corridor. 
* *SafeGraph global Features*:  These are variables provided by SafeGraph to explain visitor behavior at various points of interest. This includes the average distance from home a visitor travels to a destination  or how long they spend at a destination. These have been aggregated up to the corridor level.
* *Normalized Count Features*: These features normalize the number of amenities (business types, institutions) by the area of a given corridor.
* *Nearest Neighbor Features*: These features capture nearest neighbor distance to other Philadelphia amenities, such as parks, transit and Center City.
* *Demographic Features*: Pulled from the census, these features capture racial, and socio economic characteristics at the corridor level.

Next we wrangle the data to aggregate it at the corridor level. To start, we are looking at aggregated SafeGraph variables, such as median dwell time and average distance from home. 

We also review the share of certain business types and characteristics along a specific corridor. These variables count the SafeGraph points of interest that fit into given category (bars, restaurants, college) and divide it by hte total number of businesses along the corridor. We also divide by 12 since the data is disaggregated by month.

Finally, this code generates nearest neighbor variables for business by corridor.
```{r Observe correlations}
#Engineer variables at the destination level.
# summary(dat_pred)
dat_pred <-
  dat_pred_temp %>%
  mutate(total = 1,
         bars = ifelse(top_category == 'Drinking Places (Alcoholic Beverages)', 1, 0),
         restaurant = ifelse(top_category == 'Restaurants and Other Eating Places', 1, 0),
         arts = ifelse(top_category == 'Promoters of Performing Arts, Sports, and Similar Events' |
                         top_category == 'Performing Arts Companies', 1, 0),
         restaurant = ifelse(top_category == 'Restaurants and Other Eating Places', 1, 0),
         grocery = ifelse(top_category == 'Grocery Stores', 1, 0),
         childcare = ifelse(top_category == "Child Day Care Services", 1, 0),
         gas = ifelse(top_category == "Gasoline Stations", 1, 0),
         religious = ifelse(top_category == "Religious Organization", 1, 0),
         personal = ifelse(top_category == "Personal Care Services", 1, 0),
         liquor = ifelse(top_category == "Beer, Wine, and Liquor Stores", 1, 0),
         amusement = ifelse(top_category == 'Other Amusement and Recreation Industries', 1, 0),
         college = ifelse(top_category == 'Colleges, Universities, and Professional Schools', 1, 0),
         sports = ifelse(top_category == 'Spectator Sports', 1, 0),
         museum = ifelse(top_category == 'Museums, Historical Sites, and Similar Institutions', 1, 0),
         hotels = ifelse(top_category == 'Traveler Accommodation', 1, 0),
         transit_nn = nn_function(st_c(st_centroid(dat_pred_temp)), st_c(st_centroid(septaStops)), 2),
         bars_nn = nn_function(st_c(st_centroid(dat_pred_temp)), st_c(st_centroid(bar.sf)), 4),
         rest_nn = nn_function(st_c(st_centroid(dat_pred_temp)), st_c(st_centroid(restaurant.sf)), 4),
         arts_nn = nn_function(st_c(st_centroid(dat_pred_temp)), st_c(st_centroid(arts.sf)), 4),
         college_nn = nn_function(st_c(st_centroid(dat_pred_temp)), st_c(st_centroid(college.sf)), 1),
         sports_nn = nn_function(st_c(st_centroid(dat_pred_temp)), st_c(st_centroid(sports.sf)), 1),
         hotels_nn = nn_function(st_c(st_centroid(dat_pred_temp)), st_c(st_centroid(hotels.sf)), 4),
         casinos_nn = nn_function(st_c(st_centroid(dat_pred_temp)), st_c(st_centroid(casinos.sf)), 1),
         parks_nn = nn_function(st_c(st_centroid(dat_pred_temp)), st_c(st_centroid(parks)), 4),
         # parking_nn = nn_function(st_c(st_centroid(dat_pred_temp)), st_c(st_centroid(parking.sf)), 1),
         busstop_nn = nn_function(st_c(st_centroid(dat_pred_temp)), st_c(st_centroid(phl_busstop)), 4),
         trolley_nn = nn_function(st_c(st_centroid(dat_pred_temp)), st_c(st_centroid(phl_trolley)), 4),
         late_night = if_else(grepl("Late Night", dat_pred_temp$category_tags), 1, 0),
         bar_pub = if_else(grepl("Bar or Pub", dat_pred_temp$category_tags), 1, 0),
         drinks = if_else(grepl("Drinks", dat_pred_temp$category_tags), 1, 0),
         cocktail = if_else(grepl("Cocktail Lounge", dat_pred_temp$category_tags), 1, 0),
         SocHill = if_else(neighborhood == "Society Hill", 1, 0),
         UCity = if_else(neighborhood == "University City", 1, 0),
         NavyYard = if_else(neighborhood == "Navy Yard", 1, 0),
         PM_Workday = Hrs19_0 / Hrs_workday,
         AM_PM = (Hrs1_6 + Hrs7_12) / (Hrs13_18+Hrs19_0),
         restaurant_sqft = ifelse(top_category == "Restaurants and Other Eating Places", bld_area_sqft, 0),
         bars_sqft = ifelse(top_category == "Drinking Places (Alcoholic Beverages)", bld_area_sqft, 0),
         closing_time = ifelse(str_detect(open_hours, paste(c("20:00", "21:00", "22:00", "23:00", "0:00", "1:00", "2:00", "3:00"), collapse = "|")), "OPEN LATE", "NOT OPEN LATE"),
         closing_time = ifelse(is.na(closing_time) == TRUE, "NO DATA", closing_time),
         open_late = ifelse(closing_time == "OPEN LATE", 1, 0)
	)

dat_pred$CenterCity <- as.numeric(st_distance(dat_pred, CenterCity.sf))
dat_pred$CityHall <- as.numeric(st_distance(dat_pred, CityHall.sf))
dat_pred$Temple <- as.numeric(st_distance(dat_pred, Temple.sf))
dat_pred$UPenn <- as.numeric(st_distance(dat_pred, UPenn.sf))

#Aggregating by corridor
# summary(dat_pred_agg)
dat_pred_agg <- 
  dat_pred %>%
  group_by(corridor, date_range_start, corr_type) %>%
  summarize(Night_visits = sum(Hrs19_0),
            Workday_visits = sum(Hrs_workday),
            phl_area_sqmi = mean(corr_area_sqmi, na.rm = TRUE),
            Night_visits_sqmi = (Night_visits + 1) / phl_area_sqmi,
            Night_visits_sqmi_log = log(Night_visits_sqmi),
            Workday_visits_sqmi = (Workday_visits +1) / phl_area_sqmi,
            Workday_visits_sqmi_log = log(Workday_visits_sqmi),
            total = sum(total),
            phl_building_size = mean(bld_area_sqft +1, na.rm = TRUE),
            phl_building_size_log = log(phl_building_size),
            phl_building_size_med = median(bld_area_sqft, na.rm = TRUE),
            # phl_CenterCity = mean(CenterCity),
            # phl_CityHall = mean(CityHall),
            # phl_CityHall_log = log(phl_CityHall),
            # phl_Temple = mean(Temple),
            phl_UPenn = mean(UPenn),
            # phl_vacrate = mean(vacancy_rate + 1),
            # phl_vacrate_log = log(phl_vacrate),
            phl_rest_sqft = mean(restaurant_sqft +1,na.rm = T),
            phl_rest_sqft_log = log(phl_rest_sqft),
            phl_bar_sqft = mean(bars_sqft +1, na.rm = T),
            phl_bar_sqft_log = log(phl_bar_sqft),
            # sg_visitors = sum(raw_visitor_counts),
            # sg_visits = sum(raw_visit_counts),
            sg_dwell = mean(median_dwell, na.rm = TRUE),
            sg_distance_home = mean(distance_from_home, na.rm = TRUE),
            # sg_workday = mean(PM_Workday, na.rm = TRUE),
            # sg_AM_PM = mean(AM_PM, na.rm = TRUE),
            sg_distance_home_log = log(sg_distance_home),
            count_bars_a = (sum(bars, na.rm = TRUE) + 1)/phl_area_sqmi,
            count_rest_a = (sum(restaurant, na.rm = TRUE) + 1)/phl_area_sqmi,
            count_arts_a = (sum(arts, na.rm = TRUE) +1)/phl_area_sqmi,
            # count_jrcol_a = sum(jrcol, na.rm = TRUE)/phl_area_sqmi,
            # count_college_a = (sum(college, na.rm = TRUE) + 1)/phl_area_sqmi,
            count_sports_a = (sum(sports, na.rm = TRUE) + 1)/phl_area_sqmi,
            # count_museums_a = (sum(museum, na.rm = TRUE)  + 1)/phl_area_sqmi,
            # count_amuse_a = (sum(amusement, na.rm = TRUE)  + 1)/phl_area_sqmi,
            count_hotels_a = (sum(hotels, na.rm = TRUE) + 1)/phl_area_sqmi,
            count_grocery_a = (sum(grocery, na.rm = TRUE) + 1)/phl_area_sqmi,
            # count_childcare_a = (sum(childcare, na.rm = TRUE) + 1)/phl_area_sqmi,
            # count_gas_a = (sum(gas, na.rm = TRUE) + 1)/phl_area_sqmi,
            # count_religious_a = (sum(religious, na.rm = TRUE) + 1)/phl_area_sqmi,
            # count_personal_a = (sum(personal, na.rm = TRUE) + 1)/phl_area_sqmi,
            count_retailmix_top = n_distinct(top_category),
            count_retailmix_sub = n_distinct(sub_category),
            count_late_tag = (sum(late_night, na.rm = TRUE) + 1)/phl_area_sqmi,
            # count_barpub_tag = (sum(bar_pub, na.rm = TRUE) + 1)/phl_area_sqmi,
            # count_drinks_tag = (sum(drinks, na.rm = TRUE) + 1)/phl_area_sqmi,
            # count_cocktail_tag = (sum(cocktail, na.rm = TRUE) + 1)/phl_area_sqmi,
            count_openlate = (sum(open_late, na.rm = TRUE) + 1)/phl_area_sqmi,
            count_bars_a_log = log(count_bars_a),
            count_rest_a_log = log(count_rest_a),
            count_arts_a_log = log(count_arts_a),
            count_grocery_a_log = log(count_grocery_a),
            # count_religious_a_log = log(count_religious_a),
            # count_childcare_a_log = log(count_childcare_a),
            # log_jrcol_a = log(sum(jrcol, na.rm = TRUE)/phl_area_sqmi),
            # log_college_a = log(sum(college, na.rm = TRUE)/phl_area_sqmi),
            # log_sports_a = log(sum(sports, na.rm = TRUE)/phl_area_sqmi),
            # log_museums_a = log(sum(museum, na.rm = TRUE)/phl_area_sqmi),
            # count_amuse_a_log = log(count_amuse_a),
            # count_barpub_log = log(count_barpub_tag),
            count_late_log = log(count_late_tag),
            count_retailmix_top_log = log(count_retailmix_top),
            count_retailmix_sub_log = log(count_retailmix_sub),
            count_openlate_log = log(count_openlate),
            nn_transit = mean(transit_nn),
            nn_parks = mean(parks_nn),
            # nn_parking = mean(parking_nn),
            nn_busstop = mean(busstop_nn),
            nn_trolley = mean(trolley_nn),
            # nn_bars = mean(bars_nn),
            # nn_rest = mean(rest_nn),
            # nn_arts = mean(arts_nn),
            # nn_college = mean(college_nn),
            # nn_sports = mean(sports_nn),
            # nn_casinos = mean(casinos_nn),
            # nn_hotels = mean(hotels_nn),
            nn_parks_log = log(nn_parks),
            nn_transit_log = log(nn_transit),
            # nn_parking_log = log(nn_parking),
            nn_busstop_log = log(nn_busstop),
            nn_trolley_log = log(nn_trolley),
            demo_popdens = mean(popdens_mi),
            demo_popdens_log = log(demo_popdens),
            demo_pctWhite = weighted.mean(pctWhite, Hrs19_0, na.rm = TRUE),
            demo_pctBlack = weighted.mean(pctBlack, Hrs19_0, na.rm = TRUE),
            demo_pctHisp = weighted.mean(pctHisp, Hrs19_0, na.rm = TRUE),
            demo_medAge = weighted.mean(MedAge, Hrs19_0, na.rm = TRUE),
            demo_MHI = weighted.mean(MedHHInc, Hrs19_0, na.rm = TRUE),
            demo_medrent = weighted.mean(MedRent, Hrs19_0, na.rm = TRUE),
            demo_pctHisp_log = log(demo_pctHisp + 1)) %>% 
  # mutate_if(is.numeric, list(~na_if(., Inf))) %>%
  st_drop_geometry() %>%
  left_join(phl_corridors_pred %>% select(corridor)) %>%
  drop_na(corridor, demo_MHI, demo_medrent) %>% #Some of the census variables aren't reporting for our block groups
  st_as_sf() %>%
  ungroup() %>%
  na.omit(st_distance_home)
```

## Plots

To improve the model, we've manipulated the dependent variable to normalize the distribution. We've normalized by area and performed a log transformation. The following figure shows each change.
```{r histograms, out.width = "100%"}
dat_pred_agg %>%
  st_drop_geometry() %>%
  select(Night_visits, Night_visits_sqmi, Night_visits_sqmi_log) %>%
  pivot_longer(., cols = starts_with("Night_visits"), names_to="variable", values_to="value") %>%
  ggplot(aes(value)) +
  geom_histogram() +
  labs(title = "Dependent Variable Transformation",
       subtitle = "Figure X.X") + 
  facet_wrap(~variable, scales = "free") + 
  plotTheme()
```

The following correlation plots summarize our findings. Looking first at the Phiadephia variables below, we see that the area of the corridor as well as the average size of buildings have a positive relationship with the number of nighttime visits.
```{r phl corr plots, out.width = "100%"}
#Philadelphia summary variables
dat_pred_agg %>%
  st_drop_geometry() %>%
  pivot_longer(., cols = starts_with("phl_"), names_to = "variable", values_to="value") %>%
  ggplot(aes(value, Night_visits_sqmi_log)) +
  geom_point(size = .5) +
  geom_smooth(method = "lm", se=F, colour = "#FA7800") +
  facet_wrap(~variable, ncol = 3, scales = "free") +
  labs(title = "Nighttime Visits as a function of Philadelphia Summary Variables",
       subtitle = "Figure X.X") +
  plotTheme()
# 
# dat_pred_agg %>%
#   st_drop_geometry() %>%
#   select(starts_with("phl_")) %>%
#   pivot_longer(., cols = starts_with("phl_"), names_to = "variable", values_to="value") %>%
#   ggplot(aes(value)) +
#   geom_histogram() +
#   labs(title = "Philadephia Variable distribution",
#        subtitle = "Figure X.X") + 
#   facet_wrap(~variable, scales = "free") + 
#   plotTheme()
```

Next, we observe that of the SafeGraph summary variables, the distnace from home has a strong relationship with total visits. The count of visitors also demonstrates a strong relationship, but this is likely collinear with the the number of visits.
```{r sg corr plots, out.width = "100%"}
#Safegraph summary variables
dat_pred_agg %>%
  st_drop_geometry() %>%
  pivot_longer(., cols = starts_with("sg_"), names_to = "variable", values_to="value") %>%
  ggplot(aes(value, Night_visits_sqmi_log)) +
  geom_point(size = .5) +
  geom_smooth(method = "lm", se=F, colour = "#FA7800") +
  facet_wrap(~variable, ncol = 3, scales = "free") +
  labs(title = "Nighttime Visits as a function of Gobal SafeGraph Summary Variables",
       subtitle = "Figure X.X") +
  plotTheme()
# 
# dat_pred_agg %>%
#   st_drop_geometry() %>%
#   pivot_longer(., cols = starts_with("sg_"), names_to = "variable", values_to="value") %>%
#   ggplot(aes(value)) +
#   geom_histogram() +
#   labs(title = "SafeGraph Summary Variable Distribution",
#        subtitle = "Figure X.X") + 
#   facet_wrap(~variable, scales = "free") + 
#   plotTheme()
```

The next set of plots show the relationship between the share a specific business type by corridor and the number of trips. There seems to be a relationship with businesses that could be classified as anchor institutions, such as colleges and museums, and the number of trips. These corrplots also look at businesses with a specific tag, such as a "late night" or "bar and pub". 
```{r count corrplots, out.width = "100%"}
#Count variables
dat_pred_agg %>%
  st_drop_geometry() %>%
  pivot_longer(., cols = c(starts_with("count_")), names_to = "variable", values_to="value") %>%
  ggplot(aes(value, Night_visits_sqmi_log)) +
  geom_point(size = .5) +
  geom_smooth(method = "lm", se=F, colour = "#FA7800") +
  facet_wrap(~variable, ncol = 4, scales = "free") +
  labs(title = "Nighttime Visits as a function of Normalized Count Variables",
       subtitle = "Figure X.X") +
  plotTheme()
# 
# colnames(dat_pred)
# 
# dat_pred$date_range_start

# cat_agg <- dat_pred %>%
#   filter(date_range_start == "2018-01-01T05:00:00Z") %>%
#   select(corridor, top_category, Hrs19_0, corr_area_sqmi) %>%
#   group_by(corridor, top_category) %>%
#   summarize(night_visits = sum(Hrs19_0),
#             corr_area_sqmi = mean(corr_area_sqmi),
#             night_visits_sqmi = night_visits / corr_area_sqmi,
#             nigh_visits_sqmi_log = log(night_visits_sqmi),
#             bus_count = n(),
#             bus_count_a = bus_count + 1 / corr_area_sqmi,
#             bus_count_a_log = log(bus_count_a)) %>%
#   st_drop_geometry()
# 
# # cat_agg_long <- cat_agg %>%
# #   pivot_longer(., cols = c("Night_visits", "Bus_count"),
# #                names_to = "variable",
# #                values_to = "value") %>%
# #   select(corridor, top_category, variable, value)
# 
# categories <- unique(cat_agg$top_category)
# 
# cat_1 <- categories[1:25]
# cat_2 <- categories[26:50]
# cat_3 <- categories[51:75]
# cat_4 <- categories[76:100]
# cat_5 <- categories[101:127]
# 
# cat_agg %>%
#   filter(top_category %in% cat_1) %>%
#   ggplot(aes(bus_count_a_log, nigh_visits_sqmi_log)) +
#   geom_point(size = .5) +
#   geom_smooth(method = "lm", se=F, colour = "#FA7800") +
#   facet_wrap(~top_category, scales = "free_y") +
#   plotTheme()
# 
# cat_agg %>%
#   filter(top_category %in% cat_2) %>%
#   ggplot(aes(bus_count_a_log, nigh_visits_sqmi_log)) +
#   geom_point(size = .5) +
#   geom_smooth(method = "lm", se=F, colour = "#FA7800") +
#   facet_wrap(~top_category, scales = "free_y") +
#   plotTheme()
# 
# cat_agg %>%
#   filter(top_category %in% cat_3) %>%
#   ggplot(aes(bus_count_a_log, nigh_visits_sqmi_log)) +
#   geom_point(size = .5) +
#   geom_smooth(method = "lm", se=F, colour = "#FA7800") +
#   facet_wrap(~top_category, scales = "free_y") +
#   plotTheme()
# 
# cat_agg %>%
#   filter(top_category %in% cat_4) %>%
#   ggplot(aes(bus_count_a_log, nigh_visits_sqmi_log)) +
#   geom_point(size = .5) +
#   geom_smooth(method = "lm", se=F, colour = "#FA7800") +
#   facet_wrap(~top_category, scales = "free_y") +
#   plotTheme()
# 
# cat_agg %>%
#   filter(top_category %in% cat_5) %>%
#   ggplot(aes(bus_count_a_log, nigh_visits_sqmi_log)) +
#   geom_point(size = .5) +
#   geom_smooth(method = "lm", se=F, colour = "#FA7800") +
#   facet_wrap(~top_category, scales = "free_y") +
#   plotTheme()

# dat_pred_agg %>%
#   st_drop_geometry() %>%
#   select(starts_with("count_")) %>%
#   pivot_longer(., cols = starts_with("count_openlate"), names_to = ,"variable", values_to="value") %>%
#   ggplot(aes(value)) +
#   geom_histogram() +
#   labs(title = "Count Variable Distribution",
#        subtitle = "Figure X.X") + 
#   facet_wrap(~variable, scales = "free", ncol = 4) + 
#   plotTheme()
```

We also generated nearest neighbor variables for the corridors based on the average nearest neighbor variable for each of the destinations along a corridor. Though not as strong as the other variables we have reviewed above, these do demonstrate a negative relationship indicating that the further away a corridor is from these amenities, the less traffic to a corridor.
```{r nn corrplots, out.width = "100%"}
#Nearest neighbor variables
dat_pred_agg %>%
  st_drop_geometry() %>%
  pivot_longer(., cols = starts_with("nn_"), names_to = "variable", values_to="value") %>%
  ggplot(aes(value, Night_visits_sqmi_log)) +
  geom_point(size = .5) +
  geom_smooth(method = "lm", se=F, colour = "#FA7800") +
  facet_wrap(~variable, ncol = 3, scales = "free") +
  labs(title = "Nighttime Visits as a function of Nearest Neighbor Variables",
       subtitle = "Figure X.X") +
  plotTheme()

# dat_pred_agg %>%
#   st_drop_geometry() %>%
#   select(starts_with("nn_")) %>%
#   pivot_longer(., cols = starts_with("nn_"), names_to = ,"variable", values_to="value") %>%
#   ggplot(aes(value)) +
#   geom_histogram() +
#   labs(title = "NN Variable Distribution",
#        subtitle = "Figure X.X") + 
#   facet_wrap(~variable, scales = "free") + 
#   plotTheme()
```

Finally, we created demographic variables from census data. We pulled census information, joined it to the individual destinations and then aggregated the variables by commercial corridor. The median rent variable appears to have the strongest relationship with the depedent nighttime traffic variable.
```{r census corrplot, out.width = "100%"}
##Demographic Variables
dat_pred_agg %>%
  st_drop_geometry() %>%
  pivot_longer(., cols = starts_with("demo_"), names_to = "variable", values_to="value") %>%
  ggplot(aes(value, Night_visits_sqmi_log)) +
  geom_point(size = .5) +
  geom_smooth(method = "lm", se=F, colour = "#FA7800") +
  facet_wrap(~variable, ncol = 3, scales = "free") +
  labs(title = "Nighttime Visits as a function of Demographic Variables",
       subtitle = "Figure X.X") +
  plotTheme()

# dat_pred_agg %>%
#   st_drop_geometry() %>%
#   pivot_longer(., cols = starts_with("demo_"), names_to = ,"variable", values_to="value") %>%
#   ggplot(aes(value)) +
#   geom_histogram() +
#   labs(title = "NN Variable Distribution",
#        subtitle = "Figure X.X") + 
#   facet_wrap(~variable, scales = "free") + 
#   plotTheme()
```

## Baseline Linear Model

### Feature Summary

Using the features we engineered above, we split our data into a training and test set and run a linear regression model in the following code block. The results of the regression are shown below.
```{r model building}
#Setting up test and training datasets
set.seed(414)
inTrain <- createDataPartition(y=dat_pred_agg$Night_visits_sqmi_log, p = .60, list = FALSE)

phl.training <- dat_pred_agg[inTrain,]
phl.test <- dat_pred_agg[-inTrain,]

reg.vars <- c('Night_visits_sqmi_log',
              'phl_building_size_log',
              'phl_UPenn',
              # 'phl_Temple',
              # 'phl_CityHall',
              # 'phl_CityHall_log',
              # 'phl_rest_sqft_log',
              # 'phl_bar_sqft_log',
              # 'phl_vacrate_log',
              # 'phl_CenterCity',
              'sg_distance_home_log',
              'sg_dwell',
              'count_bars_a_log',
              'count_rest_a_log',
              'count_arts_a_log',
              'count_grocery_a_log',
              # 'count_childcare_a_log',
              # 'count_religious_a_log',
              # 'count_bars_a',
              # 'count_rest_a',
              # 'count_arts_a',
              # 'count_college_a',
              'count_sports_a',
              'count_retailmix_top',
              'count_retailmix_sub_log',
              'count_openlate_log',
              # 'count_museums_a',
              # 'count_amuse_a_log',
              # 'count_hotels_a',
              'nn_transit_log',
              # 'nn_parking',
              'nn_parks_log',
              # 'nn_busstop',
              'nn_trolley',
              'count_late_tag',
              # 'count_barpub_tag',
              'corr_type',
              'count_late_log',
              # 'count_barpub_log',
              'demo_pctWhite',
              # 'demo_pctBlack',
              # 'demo_pctHisp_log',
              'demo_medAge',
              'demo_popdens',
              # 'demo_popdens_log',
              'demo_medrent',
              'demo_MHI')

#Multivariate regression
reg1 <- 
  lm(Night_visits_sqmi_log ~ ., #change lm to ranger, predictions are a little different
     data = st_drop_geometry(phl.training) %>% 
       select(reg.vars))
             
summary(reg1)
```

### Model Errors

Both models' errors are summarized in the following table. Currently we see that the subset of data does not have a particularly noticeable impact on the model's accuracy.

```{r, out.width = "100%"}
phl.test <-
  phl.test %>%
  st_drop_geometry() %>%
  mutate(Regression = "Baseline Regression",
         Visits.Predict = exp(predict(reg1, phl.test)),
         Visits.Error = Visits.Predict - Night_visits_sqmi,
         Visits.AbsError = abs(Visits.Predict - Night_visits_sqmi),
         Visits.APE = (abs(Visits.Predict - Night_visits_sqmi)) / Visits.Predict)

ErrorTable <-
  phl.test %>%
  dplyr::summarize(Regression = "Baseline Regression",
                   MAE = mean(Visits.AbsError, na.rm = T),
                   MAPE = mean(Visits.AbsError, na.rm = T) / mean(Night_visits_sqmi, na.rm = T))

ErrorTable %>% 
  group_by(Regression) %>%
  arrange(desc(MAE)) %>% 
  kable(caption = "MAE and MAPE for Test Set Data") %>% kable_styling()
```

The following chart compares the predicted values to the actual values for the training and test sets for the full dataset. The predicted value (green line) actually lines up pretty well with the actual value (orange line). We are underpredicting in both the training and the tests sets. 
```{r pred actual scatterplot, out.width = "100%"}
reg1_predict <- exp(predict(reg1, newdata = phl.test))

rmse.train <- caret::MAE(exp(predict(reg1)), phl.training$Night_visits_sqmi)
rmse.test <- caret::MAE(reg1_predict, phl.test$Night_visits_sqmi)

preds.train <- data.frame(pred   = exp(predict(reg1)),
                          actual = phl.training$Night_visits_sqmi,
                          source = "training data")
preds.test  <- data.frame(pred   = reg1_predict,
                          actual = phl.test$Night_visits_sqmi,
                          source = "testing data")

preds <- rbind(preds.train, preds.test)

ggplot(preds, aes(x = pred, y = actual, color = source)) +
  geom_point() +
  geom_smooth(method = "lm", color = "green") +
  geom_abline(color = "orange") +
  theme_bw() +
  coord_equal() +
  facet_wrap(~source, ncol = 2) +
  labs(title = "Comparing predictions to actual values",
       x = "Predicted Value",
       y = "Actual Value",
       subtitle = "Figure X.X") +
  theme(
    legend.position = "none"
  )
```

### Cross Validation

We also test for generalizability with cross validation for the full dataset with the below code. The plot shows the Mean Average Error across 100 folds. 
```{r, out.width = "100%"}
fitControl <- trainControl(method = "cv", 
                           number = 10,
                           savePredictions = TRUE)

set.seed(414)
reg1.cv <- 
  train(Night_visits_sqmi_log ~ ., data = st_drop_geometry(dat_pred_agg) %>% 
          dplyr::select(reg.vars), 
        method = "lm", 
        trControl = fitControl, 
        na.action = na.pass)

reg1.cv.resample <- reg1.cv$resample

reg1.cv 
ErrorHist <- 
  ggplot(reg1.cv.resample, aes(x=MAE)) + 
  geom_histogram(color = "grey40", fill = "#27fdf5", bins = 50) + 
  labs(title="Linear Model: Histogram of Mean Average Error Across 100 Folds",
       subtitle = "Figure X.X") +
  plotTheme()

ErrorHist
```

### Mapping Errors & Errors by Corridor Type

Finally, we map the errors by commercial corridor in the following figures. The absolute errors indicate that our model is less accurate predicting nighttime traffic for larger, more heavily-trafficked corridors. The Percent Errors map on the right, actually tells a slightly different story with the highest quintile of percent error along the smaller corridors throughout the city. 
```{r, out.width = "100%"}
cv_preds <- reg1.cv$pred

map_preds <- dat_pred_agg %>% 
  rowid_to_column(var = "rowIndex") %>% 
  left_join(cv_preds, by = "rowIndex") %>%
  group_by(corridor, corr_type) %>% #CONFIRM THIS APPROACH WITH MATT!
  summarise(Night_visits_sqmi = mean(Night_visits_sqmi),
            pred = mean(pred)) %>% 
  mutate(Visits.AbsError = abs(exp(pred) - Night_visits_sqmi),
         PercentError = (Visits.AbsError / Night_visits_sqmi)*100) 

#Quintile maps
ErrorPlot1 <- ggplot() +
  geom_sf(data = phl_boundary, fill = "grey40") +
  geom_sf(data = map_preds, aes(fill = q5(Visits.AbsError)), color = "transparent") +
  scale_fill_manual(values = palette5,
                      labels=qBr(map_preds,"Visits.AbsError"),
                      name="Quintile\nBreaks") +
  labs(title="Absolute Errors",
       subtitle = "Night Visit Predictions",
       caption = "Figure X.X") +
  mapTheme()

ErrorPlot2 <- ggplot() +
  geom_sf(data = phl_boundary, fill = "grey40") +
  geom_sf(data = map_preds, aes(fill = q5(PercentError)), color = "transparent") +
  scale_fill_manual(values = palette5,
                      labels=qBr(map_preds, "PercentError"),
                      name="Quintile\nBreaks") +
  labs(title="Percent Errors",
       subtitle = "Night Visit Predictions",
       caption = "Figure X.X") +
  mapTheme()

grid.arrange(ErrorPlot1, ErrorPlot2, ncol=2)

#Non quintile errors
ErrorPlot3 <- ggplot() +
  geom_sf(data = phl_boundary, fill = "grey40") +
  geom_sf(data = map_preds, aes(fill = Visits.AbsError), color = "transparent") +
  scale_fill_viridis() +
  labs(title="Absolute Errors",
       subtitle = "Night Visit Predictions",
       caption = "Figure X.X") +
  mapTheme()

ErrorPlot4 <- ggplot() +
  geom_sf(data = phl_boundary, fill = "grey40") +
  geom_sf(data = map_preds, aes(fill = PercentError), color = "transparent") +
  scale_fill_viridis() +
  labs(title="Percent Errors",
       subtitle = "Night Visit Predictions",
       caption = "Figure X.X") +
  mapTheme()

grid.arrange(ErrorPlot3, ErrorPlot4, ncol=2)
```

The following table summarizes error by corridor type.
```{r}
map_preds %>% 
  st_drop_geometry() %>%
  group_by(corr_type) %>%
  summarize(Avg.Visits.AbsError = mean(Visits.AbsError),
            Avg.PercentError = mean(PercentError)) %>%
  kable(caption = "Errors by corridor type") %>%
  kable_styling()
```

Finally, the below figures split out errors by corridor type.
```{r, out.width = "100%"}
map_preds %>% 
  # left_join(., st_drop_geometry(phl_corridors_pred), by = "corridor") %>%
  # drop_na(corr_type) %>%
  st_as_sf() %>%
  ggplot() +
  geom_sf(data = phl_boundary, fill = 'grey60', color = "transparent") +
  geom_sf(aes(fill = q5(PercentError), color = q5(PercentError))) +
  scale_fill_manual(values = palette5,
                      labels=qBr(map_preds, "PercentError"),
                      name="Quintile\nBreaks") +
    scale_colour_manual(values = palette5,
                      labels=qBr(map_preds, "PercentError"),
                      name="Quintile\nBreaks") +
  labs(title="Percent Errors by Corridor Type",
       subtitle = "Night Visit Predictions",
       caption = "Figure X.X") +
  mapTheme() +
  facet_wrap(~corr_type)
```

```{r}
map_preds %>% 
  # left_join(., st_drop_geometry(phl_corridors_pred), by = "corridor") %>%
  # drop_na(corr_type) %>%
  st_as_sf() %>%
  ggplot() +
  geom_sf(data = phl_boundary, fill = 'grey60', color = "transparent") +
  geom_sf(aes(fill = PercentError, color = PercentError)) +
  scale_fill_viridis() +
  scale_color_viridis() +
  labs(title="Percent Errors by Corridor Type",
       subtitle = "Night Visit Predictions",
       caption = "Figure X.X") +
  mapTheme() +
  facet_wrap(~corr_type)
```

## Linear Model Removing Outliers

### Feature Summary

In an effort to improve our Adjust R Square value, the measure indicating how well our model fits the data, we are trying to remove outliers in the data.  The following box plot shows the the distribution of the dependent variable (the log of nighttime visits normalized by corridor area) and indicates where there are outliers. 

The following code removes the Market East corridor and any observations with over 500,000 night visits per square mile.
```{r Outliers}
dat_pred_agg_sub <- dat_pred_agg[dat_pred_agg$corridor!="Market East" &
                                 dat_pred_agg$Night_visits_sqmi<500000,]

set.seed(414)
inTrain.sub <- createDataPartition(y=dat_pred_agg_sub$Night_visits_sqmi_log, p = .60, list = FALSE)

phl.training.sub <- dat_pred_agg_sub[inTrain.sub,]
phl.test.sub <- dat_pred_agg_sub[-inTrain.sub,]

#Multivariate regression
reg1.sub <- 
  lm(Night_visits_sqmi_log ~ ., #change lm to ranger, predictions are a little different
     data = st_drop_geometry(phl.training.sub) %>%
             select(reg.vars))

summary(reg1.sub)
```

### Errors
Next we compare the errors in the model removing outliers to the baseline model. There is a small improvement with the outlier observations removed.
```{r}
#Error Table
phl.test.sub <-
  phl.test.sub %>%
  st_drop_geometry() %>%
  mutate(Regression = "Linear Regression Removing Outliers",
         Visits.Predict = exp(predict(reg1.sub, phl.test.sub)),
         Visits.Error = Visits.Predict - Night_visits_sqmi,
         Visits.AbsError = abs(Visits.Predict - Night_visits_sqmi),
         Visits.APE = (abs(Visits.Predict - Night_visits_sqmi)) / Visits.Predict)

rbind(phl.test, phl.test.sub) %>%
  group_by(Regression) %>%
  dplyr::summarize(MAE = mean(Visits.AbsError, na.rm = T),
                   MAPE = mean(Visits.AbsError, na.rm = T) / mean(Night_visits_sqmi, na.rm = T)) %>% 
  kable(caption = "MAE and MAPE for Test Set Data") %>% kable_styling()
```

The following scatterplots also indicate the data is more heterogenous than before, but in the subsset data, we are under-predicting more than in the full data model.
```{r}
#Error Scatterplot
reg1.sub_predict <- exp(predict(reg1.sub, newdata = phl.test.sub))

rmse.train <- caret::MAE(exp(predict(reg1.sub)), phl.training.sub$Night_visits_sqmi)
rmse.test <- caret::MAE(reg1.sub_predict, phl.test.sub$Night_visits_sqmi)

preds.train.sub <- data.frame(pred   = exp(predict(reg1.sub)),
                          actual = phl.training.sub$Night_visits_sqmi,
                          source = "training data")
preds.test.sub  <- data.frame(pred   = reg1.sub_predict,
                          actual = phl.test.sub$Night_visits_sqmi,
                          source = "testing data")

preds.sub <- rbind(preds.train.sub, preds.test.sub)

ggplot(preds.sub, aes(x = pred, y = actual, color = source)) +
  geom_point() +
  geom_smooth(method = "lm", color = "green") +
  geom_abline(color = "orange") +
  theme_bw() +
  coord_equal() +
  facet_wrap(~source, ncol = 2) +
  labs(title = "Comparing predictions to actual values",
       x = "Predicted Value",
       y = "Actual Value",
       subtitle = "Figure X.X") +
  theme(
    legend.position = "none"
  )
```

## Random Forest Model

#Feature Selection

To cope with the non-linearity in the model, we also tried the same features in a random forest model. In the below summary table, we see much improvement in the error terms (22% down from about 40%) and a .7 R squared.
```{r ranger}
set.seed(414)
inTrain <- createDataPartition(y=dat_pred_agg$Night_visits_sqmi_log, p = .60, list = FALSE)

phl.training.rf <- dat_pred_agg[inTrain,]
phl.test.rf <- dat_pred_agg[-inTrain,]

#Multivariate regression
rf1 <- 
  ranger(Night_visits_sqmi_log ~ ., #change lm to ranger, predictions are a little different
     data = st_drop_geometry(phl.training.rf) %>%
             select(reg.vars),
     importance = "impurity")

rf1
```

The below Figure shows the importance of the various features in the model, with the density of restauratns, the businesses open late, the distance from home and the population density being the most important features.
```{r}
#Importance bar plot
rf1_importance <-
  importance(rf1) %>%
  as.data.frame() %>%
  setNames(c("importance")) %>%
  rownames_to_column(., "variable") %>%
  as.data.frame() %>%
  mutate(importance = as.vector(importance)) 

# rf1_importance <- rf1_importance[order(rf1_importance$importance, decreasing = TRUE),]
# rf1_importance$importance <- factor(rf1_importance$importance, levels = rf1_importance$importance)

rf1_importance %>% 
  ggplot(aes(x=variable, y=importance)) +
  geom_col() + 
  plotTheme() +
  labs(title = "Relative Importance for Random Forest Model Features",
       subtitle = "Figure X.X") +
  coord_flip()
```

### Error Terms

The below plots indicate that the errors are far more heteroschedastic than the previous models, but there seems to be a larger gap between the predicted and actual values as indicated by the orange and green lines. Suggestions on how to better align these?
```{r}
#######
#Scatterplot
rf1_predict.train <- predict(rf1, data = st_drop_geometry(phl.training.rf))
rf1_predict.train <- exp(rf1_predict.train$predictions)

rf1_predict.test <- predict(rf1, data = st_drop_geometry(phl.test.rf))
rf1_predict.test <- exp(rf1_predict.test$predictions)

rmse.train.rf <- caret::MAE(rf1_predict.train, phl.training.rf$Night_visits_sqmi)
rmse.test.rf <- caret::MAE(rf1_predict.test, phl.test.rf$Night_visits_sqmi)

preds.train.rf <- data.frame(pred   = rf1_predict.train,
                          actual = phl.training.rf$Night_visits_sqmi,
                          source = "training data")
preds.test.rf  <- data.frame(pred   = rf1_predict.test,
                          actual = phl.test.rf$Night_visits_sqmi,
                          source = "testing data")

preds.rf <- rbind(preds.train.rf, preds.test.rf)

ggplot(preds.rf, aes(x = pred, y = actual, color = source)) +
  geom_point() +
  geom_smooth(method = "lm", color = "green") +
  geom_abline(color = "orange") +
  theme_bw() +
  coord_equal() +
  facet_wrap(~source, ncol = 2) +
  labs(title = "Comparing predictions to actual values",
       x = "Predicted Value",
       y = "Actual Value",
       subtitle = "Figure X.X") +
  theme(
    legend.position = "none"
  )
```

The below table compares  the error terms of the random forest model to the previous two models, we see a noticeable improvement.
```{r}
#Error Table
phl.test.rf <-
  phl.test.rf %>%
  st_drop_geometry() %>%
  mutate(Regression = "Random Forest Regression",
         # Visits.Predict = exp(predict(reg1.sub, phl.test.sub)),
          Visits.Predict = rf1_predict.test,
         Visits.Error = Visits.Predict - Night_visits_sqmi,
         Visits.AbsError = abs(Visits.Predict - Night_visits_sqmi),
         Visits.APE = (abs(Visits.Predict - Night_visits_sqmi)) / Visits.Predict)

rbind(phl.test, phl.test.sub, phl.test.rf) %>%
  group_by(Regression) %>%
  dplyr::summarize(MAE = mean(Visits.AbsError, na.rm = T),
                   MAPE = mean(Visits.AbsError, na.rm = T) / mean(Night_visits_sqmi, na.rm = T)) %>% 
  kable(caption = "MAE and MAPE for Test Set Data") %>% kable_styling()
```

### Cross Validation

We also see improvement in the error terms in the below histogram comparing the mean average error in the linear model to the random forest model.

```{r}
fitControl <- trainControl(method = "cv", 
                           number = 10,
                           savePredictions = TRUE,
                           search = "grid")
?trainControl

set.seed(414)

tgrid <- expand.grid(
  mtry = 2:7,
  splitrule = "gini",
  min.node.size = c(5, 10, 20))


reg1.cv.rf <- 
  train(Night_visits_sqmi_log ~ ., data = st_drop_geometry(dat_pred_agg) %>% 
          dplyr::select(reg.vars), 
        method = "ranger", 
        trControl = fitControl,
        na.action = na.pass,
        tuneGrid = tgrid)

reg1.cv.rf.resample <- reg1.cv.rf$resample

reg1.cv.rf
# ggplot(reg1.cv.rf.resample, aes(x=MAE)) + 
#   geom_histogram(color = "grey40", fill = "#27fdf5", bins = 50) + 
#   labs(title="Full Dataset: Histogram of Mean Average Error Across 100 Folds",
#        subtitle = "Figure X.X") +
#   plotTheme()

#Comparing random forest model and linear model errors.
rbind(reg1.cv.resample %>% mutate(Regression = "Linear Model"), 
        reg1.cv.rf.resample %>% mutate(Regression = "Random Forest Model")) %>%
  ggplot(aes(x=MAE)) + 
  geom_histogram(color = "grey40", fill = "#27fdf5", bins = 50) + 
  labs(title="Full Dataset: Histogram of Mean Average Error Across 100 Folds",
       subtitle = "Figure X.X") +
  facet_wrap(~Regression, ncol = 1) +
  plotTheme()

```

## Random Forest Model Removing Outliers

### Feature Summary

We know that the random forest model predicted on the full dataset fare more accurately than the linear model. How will it perform on the data subset we prepared above? The following code runs the random forest model on that subset.

```{r}
# dat_pred_agg %>%
#   filter(Night_visits_sqmi > 750000) %>%
#   select(corridor) %>%
#   ggplot()+
#   geom_sf(data=phl_boundary) +
#   geom_sf(fill = "red", color = "red") +
#   mapTheme()
# 
# dat_pred_agg_sub_rf <- 
#   dat_pred_agg %>%
#   filter(Night_visits_sqmi < 600000)

#Random forest with subset
set.seed(414)
inTrain <- createDataPartition(y=dat_pred_agg_sub$Night_visits_sqmi_log, p = .60, list = FALSE)

phl.training.rf.sub <- dat_pred_agg_sub[inTrain,]
phl.test.rf.sub <- dat_pred_agg_sub[-inTrain,]

#Multivariate regression
rf1.sub <- 
  ranger(Night_visits_sqmi_log ~ ., #change lm to ranger, predictions are a little different
     data = st_drop_geometry(phl.training.rf.sub) %>%
             select(reg.vars),
     importance = "impurity")

rf1.sub
```

### Errors

```{r}
#Scatterplot
rf1_predict.train.sub <- predict(rf1.sub, data = st_drop_geometry(phl.training.rf.sub))
rf1_predict.train.sub <- exp(rf1_predict.train.sub$predictions)

rf1_predict.test.sub <- predict(rf1.sub, data = st_drop_geometry(phl.test.rf.sub))
rf1_predict.test.sub <- exp(rf1_predict.test.sub$predictions)

rmse.train.rf.sub <- caret::MAE(rf1_predict.train.sub, phl.training.rf.sub$Night_visits_sqmi)
rmse.test.rf.sub <- caret::MAE(rf1_predict.test.sub, phl.test.rf.sub$Night_visits_sqmi)

preds.train.rf.sub <- data.frame(pred   = rf1_predict.train.sub,
                          actual = phl.training.rf.sub$Night_visits_sqmi,
                          source = "training data")
preds.test.rf.sub  <- data.frame(pred   = rf1_predict.test.sub,
                          actual = phl.test.rf.sub$Night_visits_sqmi,
                          source = "testing data")

preds.rf.sub <- rbind(preds.train.rf.sub, preds.test.rf.sub)

ggplot(preds.rf.sub, aes(x = pred, y = actual, color = source)) +
  geom_point() +
  geom_smooth(method = "lm", color = "green") + #Is the lm here right?
  geom_abline(color = "orange") +
  theme_bw() +
  coord_equal() +
  facet_wrap(~source, ncol = 2) +
  labs(title = "Comparing predictions to actual values",
       x = "Predicted Value",
       y = "Actual Value",
       subtitle = "Figure X.X") +
  theme(
    legend.position = "none"
  )
```

Comparing the error terms to the other models, we see that the subsetted random forest model actually performs slightly worse than the random forest model on the full set.

```{r}
#Error Table
phl.test.rf.sub <-
  phl.test.rf.sub %>%
  st_drop_geometry() %>%
  mutate(Regression = "Subset RF Regression",
         # Visits.Predict = exp(predict(reg1.sub, phl.test.sub)),
          Visits.Predict = rf1_predict.test.sub,
         Visits.Error = Visits.Predict - Night_visits_sqmi,
         Visits.AbsError = abs(Visits.Predict - Night_visits_sqmi),
         Visits.APE = (abs(Visits.Predict - Night_visits_sqmi)) / Visits.Predict)

rbind(phl.test, phl.test.sub, phl.test.rf, phl.test.rf.sub) %>%
  group_by(Regression) %>%
  dplyr::summarize(MAE = mean(Visits.AbsError, na.rm = T),
                   MAPE = mean(Visits.AbsError, na.rm = T) / mean(Night_visits_sqmi, na.rm = T)) %>% 
  kable(caption = "MAE and MAPE for Test Set Data") %>% kable_styling()
```

### Workday Model

Finally, we were curious to see if our model could predict traffic during other times of day. Using the same fetures, we ran an additional linear model and random forest model predicting *workday traffic* between the hours of 9AM and 6PM (*CONFIRM times*). Based on the summary table comparing the Workday models to the other models, these features are much worse at predicting daytime traffic.

```{r daytime linear model}
##Linear Workday Model
set.seed(414)
inTrain.day <- createDataPartition(y=dat_pred_agg$Workday_visits_sqmi_log, p = .60, list = FALSE)

phl.training.day <- dat_pred_agg[inTrain.day,]
phl.test.day <- dat_pred_agg[-inTrain.day,]

reg.vars.day <- c('Workday_visits_sqmi_log',
              'phl_building_size_log',
              'phl_UPenn',
              # 'phl_Temple',
              # 'phl_CityHall',
              # 'phl_CityHall_log',
              # 'phl_rest_sqft_log',
              # 'phl_bar_sqft_log',
              # 'phl_vacrate_log',
              # 'phl_CenterCity',
              'sg_distance_home_log',
              'sg_dwell',
              'count_bars_a_log',
              'count_rest_a_log',
              'count_arts_a_log',
              'count_grocery_a_log',
              # 'count_childcare_a_log',
              # 'count_religious_a_log',
              # 'count_bars_a',
              # 'count_rest_a',
              # 'count_arts_a',
              # 'count_college_a',
              'count_sports_a',
              'count_retailmix_top',
              'count_retailmix_sub_log',
              'count_openlate_log',
              # 'count_museums_a',
              # 'count_amuse_a_log',
              # 'count_hotels_a',
              'nn_transit_log',
              # 'nn_parking',
              'nn_parks_log',
              # 'nn_busstop',
              'nn_trolley',
              'count_late_tag',
              # 'count_barpub_tag',
              'corr_type',
              'count_late_log',
              # 'count_barpub_log',
              'demo_pctWhite',
              # 'demo_pctBlack',
              # 'demo_pctHisp_log',
              'demo_medAge',
              'demo_popdens',
              # 'demo_popdens_log',
              'demo_medrent',
              'demo_MHI')

#Multivariate regression
reg1.day <- 
  lm(Workday_visits_sqmi_log ~ ., 
     data = st_drop_geometry(phl.training.day) %>% 
       select(reg.vars.day))

#Random Forest Workday model
set.seed(414)
phl.training.rf.day <- dat_pred_agg[inTrain,]
phl.test.rf.day <- dat_pred_agg[-inTrain,]

#Multivariate regression
rf1.day <- 
  ranger(Workday_visits_sqmi_log ~ ., #change lm to ranger, predictions are a little different
     data = st_drop_geometry(phl.training.rf.day) %>%
             select(reg.vars.day),
     importance = "impurity")

rf1_predict.train.day <- predict(rf1.day, data = st_drop_geometry(phl.training.rf.day))
rf1_predict.train.day <- exp(rf1_predict.train.day$predictions)

rf1_predict.test.day <- predict(rf1.day, data = st_drop_geometry(phl.test.rf.day))
rf1_predict.test.day <- exp(rf1_predict.test.day$predictions)

#Error table
phl.test.day <-
  phl.test.day %>%
  st_drop_geometry() %>%
  mutate(Regression = "Workday Linear Regression",
         Visits.Predict = exp(predict(reg1.day, phl.test.day)),
         Visits.Error = Visits.Predict - Workday_visits_sqmi,
         Visits.AbsError = abs(Visits.Predict - Workday_visits_sqmi),
         Visits.APE = (abs(Visits.Predict - Workday_visits_sqmi)) / Visits.Predict)

phl.test.rf.day <-
  phl.test.rf.day %>%
  st_drop_geometry() %>%
  mutate(Regression = "Workday Random Forest Regression",
         # Visits.Predict = exp(predict(rf1.day, phl.test.rf.day)),
         Visits.Predict = rf1_predict.test.day,
         Visits.Error = Visits.Predict - Workday_visits_sqmi,
         Visits.AbsError = abs(Visits.Predict - Workday_visits_sqmi),
         Visits.APE = (abs(Visits.Predict - Workday_visits_sqmi)) / Visits.Predict)

rbind(phl.test, phl.test.sub, phl.test.rf.sub, phl.test.rf, phl.test.day, phl.test.rf.day) %>%
  group_by(Regression) %>%
  dplyr::summarize(MAE = mean(Visits.AbsError, na.rm = T),
                   MAPE = mean(Visits.AbsError, na.rm = T) / mean(Night_visits_sqmi, na.rm = T)) %>% 
  kable(caption = "MAE and MAPE for Test Set Data") %>% kable_styling()

```

# 5. Next Steps

For next week, we will primarily focus on preparing our presentation by doing the following:

* App development
* Write-up